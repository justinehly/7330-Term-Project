---
title: "Updated Pfizer Vaccine Tweet"
author: "Mingyang Nick YU"
date: "3/13/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## This document is focused on clean up location, text, merge with sentiment for the updated vaccineation_tweets0313.csv

#### Initial importing the data under tweets

```{r}
# You need to adjust the work environment to run this code
setwd("/Users/mingyang/Desktop/SMU/File Management & Database/7330-Term-Porject/Data Set")
library(tidyverse)
#plotting and pipes 
library(ggplot2)
# text mining library
library(tidytext)
# Read in data under tweets
tweets = read_csv("vaccination_tweets0313.csv")
str(tweets)
dim(tweets)
#Look at data
library(visdat)
vis_dat(tweets)
# Look at missing Data
library(naniar)
colSums(is.na(tweets))
gg_miss_var(tweets)

```

#### First clean text column for Amazon Comprehend analysis

```{r}
clean_text = function(string){
  #Remove URL
  string = gsub("\\s?(f|ht)(tp)(s?)(://)([^\\.]*)[\\.|/](\\S*)","", string)
  #Remove \r and \n
  string = gsub("\r?\n|\r"," ",string)
  return(string)
}

# Performance regex on tweets$text column
tweets$text = clean_text(tweets$text)
# Uniform Date Format
library(lubridate)
tweets$date = ymd_hms(tweets$date)
# Write just tweets comments column out to a csv
write.csv(tweets$text, "tweet_posts0313.csv", row.names=F)
tweet1 = read.csv("tweet_posts0313.csv",header = FALSE)
dim(tweet1)
```

#### Working on location data on State - We find city and country data less useful for our current analysis

```{r}
# Decide to fix USA after fixing STATES name first
usa.states = read_csv("usa_states.csv")
usa.states = data.frame(usa.states)
usa.states$State = toupper(usa.states$State)
usa.states = usa.states[,-2]
head(usa.states)
usstatelength = dim(usa.states)[1]
tweetslength = dim(tweets)[1]
#Make state name decending
usa.states = usa.states %>% mutate(statenamelength = nchar(State)) %>%arrange(desc(statenamelength))
tweets$user_location = toupper(tweets$user_location)
tweets$user_state = as.character(NA)

# Go through tweets for all states
for(i in 1:tweetslength){
  for(j in 1:usstatelength){
    if(grepl(usa.states[j,1],tweets[i,3])){
      print(paste("fixing tweet",i))
      tweets[i,17]=usa.states[j,1]
      print(paste("fixed",tweets[i,3],tweets[i,17]))
      break
    }
  }
}

# WASHINGTON DC fixed to WASHINGTON - should be fixed when doing " StateCode$" structure
# KANSAS CITY, MO  -- went to KANSAS - should be fixed when doing " StateCode$" structure
# Now go through City, Statecode structure
for(i in 1:tweetslength){
  for(j in 1:usstatelength){
      current.state = paste(" ",usa.states[j,2],"$",sep = "")
      if(grepl(current.state,tweets[i,3])){
        print(paste("fixing tweet",i))
        tweets[i,17]=usa.states[j,1]
        print(paste("fixed",tweets[i,3],tweets[i,17]))
        break
      }
  }
}


# WASHINGTON D.C. need to be treated as special case
for(i in 1:tweetslength){
  if((!is.na(tweets[i,17]))&(tweets[i,17]=="WASHINGTON")){
    if(grepl("D.C.",tweets[i,3])){
      print(paste("fixing tweet",i))
      tweets[i,17]=usa.states[1,1]
      print(paste("fixed",tweets[i,3],tweets[i,17]))
      break
    }
  }
}

# Fixing NYC in user_location
for(i in 1:tweetslength){
  if(grepl("NYC",tweets[i,3])&(is.na(tweets[i,17]))){
    print(paste("fixing tweet",i))
    tweets[i,17]="NEW YORK"
    print(paste("fixed",tweets[i,3],tweets[i,17]))
  }
}

#Adding state code
tweets$us_state_code= as.character(NA)
#Adding USA state code for easier merging data.
for(i in 1:tweetslength){
  if(!is.na(tweets[i,17])){
    for(j in 1:usstatelength){
      if(usa.states[j,1]==tweets[i,17]){
        print(paste("fixing tweet",i))
        tweets[i,18]=usa.states[j,2]
        print(paste("fixed",tweets[i,3],tweets[i,17],tweets[i,18]))
        break
      }
    }
  }
}
# Look at TOP US STATES
tweets %>% 
  count(user_state,sort = TRUE) %>%
  mutate(user_state = reorder(user_state, n)) %>%
  na.omit()%>%
  top_n(20)%>%
  ggplot(aes(x=user_state,y=n))+
  geom_col()+
  coord_flip()+
  labs(x = "Count",y="Location",title="Where Twitter users are from - unique State")
#location.and.state = tweets%>%filter(!is.na(user_state))%>%select(user_state,us_state_code)
#view(location.and.state)

###########################################################################################
# write out location data with US state fixed
write.csv(tweets,"fixed_location_tweets0313.csv",row.names=FALSE)
###########################################################################################
```


### Read in JSON data
### Merge with Sentiment result from AWS Comprehend(json format), export to tweets_loc_sen0223.csv

```{r}
# Read back in cleaned tweets data
tweets = read_csv("fixed_location_tweets0313.csv")
tweets = data.frame(tweets)

# Load the package required to read JSON files.
library("rjson")
library(jsonlite)
# Give the input file name to the function.
result <- stream_in(file("sentiment_0315.json"))

# Print the result.
head(result)

result$Sentiment
# Convert JSON file to a data frame.
json_data_frame <- as.data.frame(result)
# Observe the data
print(json_data_frame)
tail(json_data_frame,5)
dim(json_data_frame)

# Create new sentiment data frame frome extracting from json_data_frame
tweets$sentiment = json_data_frame$Sentiment
tweets$sentimentScore.mixed = json_data_frame$SentimentScore$Mixed
tweets$sentimentScore.negative = json_data_frame$SentimentScore$Negative
tweets$sentimentScore.neutral = json_data_frame$SentimentScore$Neutral
tweets$sentimentScore.positive = json_data_frame$SentimentScore$Positive

compare = tweets%>% select(text,sentiment)
head(compare,5)
tail(compare,5)

###########################################################################################
#another write out
write.csv(tweets,"tweets_loc_sen0315.csv",row.names=FALSE)
###########################################################################################
```













