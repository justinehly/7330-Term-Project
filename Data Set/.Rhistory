library(MASS)
library(mvtnorm)
set.seed(1234)
dataYes<-mvrnorm(30,c(10,10),matrix(c(1,.8,.8,1),2,2,byrow=T))
dataNo<- mvrnorm(30,c(10,7),matrix(c(1,.8,.8,1),2,2,byrow=T))
full<-rbind(dataYes,dataNo)
full<-data.frame(full)
full$Response<-rep(c("Yes","No"),each=30)
full$Response<-factor(full$Response)
names(full)[1:2]<-c("X1","X2")
plot(full[, 1:2], col = full$Response, main="Shift in X2")
# construct the LDA model
mylda <- lda(Response ~ X1 + X2, data = full)
# draw discrimination line
np <- 300
nd.x <- seq(from = min(full$X1), to = max(full$X1), length.out = np)
nd.y <- seq(from = min(full$X2), to = max(full$X2), length.out = np)
nd <- expand.grid(X1 = nd.x, X2 = nd.y)
prd <- as.numeric(predict(mylda, newdata = nd)$class)
plot(full[, 1:2], col = full$Response, main="Shift in X2")
points(mylda$means, pch = "+", cex = 2, col = c("black", "red"))
contour(x = nd.x, y = nd.y, z = matrix(prd, nrow = np, ncol = np),
levels = c(1, 2), add = TRUE, drawlabels = FALSE)
dataYes<-mvrnorm(30,c(10,10),matrix(c(1,.6,.6,1),2,2,byrow=T))
dataNo<- mvrnorm(30,c(8,8),matrix(c(1,.6,.6,1),2,2,byrow=T))
full<-rbind(dataYes,dataNo)
full<-data.frame(full)
full$Response<-rep(c("Yes","No"),each=30)
full$Response<-factor(full$Response)
names(full)[1:2]<-c("X1","X2")
mylda <- lda(Response ~ X1 + X2, data = full)
# draw discrimination line
np <- 300
nd.x <- seq(from = min(full$X1), to = max(full$X1), length.out = np)
nd.y <- seq(from = min(full$X2), to = max(full$X2), length.out = np)
nd <- expand.grid(X1 = nd.x, X2 = nd.y)
prd <- as.numeric(predict(mylda, newdata = nd)$class)
plot(full[, 1:2], col = full$Response, main="Shift in X2")
points(mylda$means, pch = "+", cex = 2, col = c("black", "red"))
contour(x = nd.x, y = nd.y, z = matrix(prd, nrow = np, ncol = np),
levels = c(1, 2), add = TRUE, drawlabels = FALSE)
?lds
?lda
full
# construct the LDA model
mylda <- lda(Response ~ X1 + X2, data = full,prior=c(0.8,0.2))
np <- 300
nd.x <- seq(from = min(full$X1), to = max(full$X1), length.out = np)
nd.y <- seq(from = min(full$X2), to = max(full$X2), length.out = np)
nd <- expand.grid(X1 = nd.x, X2 = nd.y)
prd <- as.numeric(predict(mylda, newdata = nd)$class)
plot(full[, 1:2], col = full$Response, main="Shift in X2")
points(mylda$means, pch = "+", cex = 2, col = c("black", "red"))
contour(x = nd.x, y = nd.y, z = matrix(prd, nrow = np, ncol = np),
levels = c(1, 2), add = TRUE, drawlabels = FALSE)
str(full$Response)
full$Response
# construct the LDA model with prior of 0.8 to Yes outcome
# Levels: No Yes, prior should be specified in the order of the vactor levels
mylda <- lda(Response ~ X1 + X2, data = full,prior=c(0.2,0.8))
np <- 300
nd.x <- seq(from = min(full$X1), to = max(full$X1), length.out = np)
nd.y <- seq(from = min(full$X2), to = max(full$X2), length.out = np)
nd <- expand.grid(X1 = nd.x, X2 = nd.y)
prd <- as.numeric(predict(mylda, newdata = nd)$class)
plot(full[, 1:2], col = full$Response, main="Incorporating Prior of Yes to 80%")
points(mylda$means, pch = "+", cex = 2, col = c("black", "red"))
contour(x = nd.x, y = nd.y, z = matrix(prd, nrow = np, ncol = np),
np <- 300
nd.x <- seq(from = min(full$X1), to = max(full$X1), length.out = np)
nd.y <- seq(from = min(full$X2), to = max(full$X2), length.out = np)
nd <- expand.grid(X1 = nd.x, X2 = nd.y)
prd <- as.numeric(predict(mylda, newdata = nd)$class)
plot(full[, 1:2], col = full$Response, main="Incorporating Prior of Yes to 80%")
points(mylda$means, pch = "+", cex = 2, col = c("black", "red"))
contour(x = nd.x, y = nd.y, z = matrix(prd, nrow = np, ncol = np),
levels = c(1, 2), add = TRUE, drawlabels = FALSE)
set.seed(1234)
dataYes<-mvrnorm(30,c(10,10),matrix(c(1,.8,.8,1),2,2,byrow=T))
dataNo<- mvrnorm(30,c(10,7),matrix(c(1,.8,.8,1),2,2,byrow=T))
full<-rbind(dataYes,dataNo)
full<-data.frame(full)
full$Response<-rep(c("Yes","No"),each=30)
full$Response<-factor(full$Response)
names(full)[1:2]<-c("X1","X2")
plot(full[, 1:2], col = full$Response, main="Shift in X2")
full
mylda <- lda(Response ~ X1 + X2, data = full)
# draw discrimination line
np <- 300
nd.x <- seq(from = min(full$X1), to = max(full$X1), length.out = np)
nd.y <- seq(from = min(full$X2), to = max(full$X2), length.out = np)
nd <- expand.grid(X1 = nd.x, X2 = nd.y)
prd <- as.numeric(predict(mylda, newdata = nd)$class)
plot(full[, 1:2], col = full$Response, main="Shift in X2")
points(mylda$means, pch = "+", cex = 2, col = c("black", "red"))
contour(x = nd.x, y = nd.y, z = matrix(prd, nrow = np, ncol = np),
levels = c(1, 2), add = TRUE, drawlabels = FALSE)
full$Response
dataYes<-mvrnorm(30,c(10,10),matrix(c(1,.6,.6,1),2,2,byrow=T))
dataNo<- mvrnorm(30,c(8,8),matrix(c(1,.6,.6,1),2,2,byrow=T))
full<-rbind(dataYes,dataNo)
full<-data.frame(full)
full$Response<-rep(c("Yes","No"),each=30)
full$Response<-factor(full$Response)
names(full)[1:2]<-c("X1","X2")
# construct the LDA model with prior of 0.8 to Yes outcome
# Levels: No Yes, prior should be specified in the order of the factor levels
mylda <- lda(Response ~ X1 + X2, data = full,prior=c(0.2,0.8))
# draw discrimination line
np <- 300
nd.x <- seq(from = min(full$X1), to = max(full$X1), length.out = np)
nd.y <- seq(from = min(full$X2), to = max(full$X2), length.out = np)
nd <- expand.grid(X1 = nd.x, X2 = nd.y)
prd <- as.numeric(predict(mylda, newdata = nd)$class)
plot(full[, 1:2], col = full$Response, main="Incorporating Prior of Yes to 80%")
points(mylda$means, pch = "+", cex = 2, col = c("black", "red"))
contour(x = nd.x, y = nd.y, z = matrix(prd, nrow = np, ncol = np),
levels = c(1, 2), add = TRUE, drawlabels = FALSE)
full[61,1]<-11
full[61,2]<-0
full[61,3]<-"Yes"
mylda <- lda(Response ~ X1 + X2, data = full)
# draw discrimination line
np <- 300
nd.x <- seq(from = min(full$X1), to = max(full$X1), length.out = np)
nd.y <- seq(from = min(full$X2), to = max(full$X2), length.out = np)
nd <- expand.grid(X1 = nd.x, X2 = nd.y)
prd <- as.numeric(predict(mylda, newdata = nd)$class)
plot(full[, 1:2], col = full$Response, main="Shift in X2")
points(mylda$means, pch = "+", cex = 2, col = c("black", "red"))
contour(x = nd.x, y = nd.y, z = matrix(prd, nrow = np, ncol = np),
levels = c(1, 2), add = TRUE, drawlabels = FALSE)
library(mvtnorm)
set.seed(1234)
muYes<-c(10,10)
muNo<-c(8,8)
Sigma<-matrix(c(1,.8,.8,1),2,2,byrow=T)
nY<-30
nN<-30
dataYes<-rmvnorm(nY,muYes,Sigma)
dataNo<- rmvnorm(nN,muNo,Sigma)
train<-rbind(dataYes,dataNo)
train<-data.frame(train)
for (i in 3:20){
train<-cbind(train,rnorm(nY+nN))
}
names(train)<-paste("X",1:20,sep="")
train$Response<-rep(c("Yes","No"),each=30)
train$Response<-factor(train$Response)
#Creating a test set
muYes<-c(10,10)
muNo<-c(8,8)
Sigma<-matrix(c(1,.8,.8,1),2,2,byrow=T)
nY<-500
nN<-500
dataYes<-rmvnorm(nY,muYes,Sigma)
dataNo<- rmvnorm(nN,muNo,Sigma)
test<-rbind(dataYes,dataNo)
test<-data.frame(test)
for (i in 3:20){
test<-cbind(test,rnorm(nY+nN))
}
names(test)<-paste("X",1:20,sep="")
test$Response<-rep(c("Yes","No"),each=500)
test$Response<-factor(test$Response)
mylda<-lda(Response~X1+X2,data=train)
pred<-predict(mylda,newdata=test)$class  #Predictions can come in many forms, the class form provides the categorical level of your response.
Truth<-test$Response
x<-table(pred,Truth) # Creating a confusion matrix
x
#Missclassification Error
ME<-(x[2,1]+x[1,2])/1000
ME
#Calculating overall accuracy
1-ME
mylda<-lda(Response~X1+X2+X3+X4+X5+X6+X7+X8+X9+X10,data=train)
pred<-predict(mylda,newdata=test)$class  #Predictions can come in many forms, the class form provides the categorical level of your response.
Truth<-test$Response
x<-table(pred,Truth) # Creating a confusion matrix
x
#Missclassification Error
ME<-(x[2,1]+x[1,2])/1000
ME
#Calculating overall accuracy
1-ME
mylda<-lda(Response~.,data=train)
pred<-predict(mylda,newdata=test)$class  #Predictions can come in many forms, the class form provides the categorical level of your response.
Truth<-test$Response
x<-table(pred,Truth) # Creating a confusion matrix
x
#Missclassification Error
ME<-(x[2,1]+x[1,2])/1000
ME
#Calculating overall accuracy
1-ME
# You need to adjust the work environment to run this code
setwd("/Users/mingyang/Desktop/SMU/File Management & Database/7330-Term-Porject/Data Set")
library(tidyverse)
# text mining library
library(tidytext)
#plotting and pipes
library(ggplot2)
str(tweets)
dim(tweets)
# Read in data under tweets
tweets = read_csv("vaccination_tweets0313.csv")
#Look at data
library(visdat)
vis_dat(tweets)
colSums(is.na(tweets))
gg_miss_var(tweets)
# Look at missing Data
library(naniar)
# Read back in cleaned tweets data
tweets = read_csv("fixed_location_tweets0313.csv")
tweets = data.frame(tweets)
# Load the package required to read JSON files.
library("rjson")
library(jsonlite)
# Give the input file name to the function.
result <- stream_in(file("sentiment_0315.json"))
# Print the result.
head(result)
results %>% arrange(Line)
result$Line
results %>% arrange(sort(Line))
result %>% arrange(sort(Line))
result %>% arrange(Line)
result = result %>% arrange(Line)
result$Sentiment
# Convert JSON file to a data frame.
json_data_frame <- as.data.frame(result)
# Observe the data
print(json_data_frame)
tail(json_data_frame,5)
dim(json_data_frame)
# Create new sentiment data frame frome extracting from json_data_frame
tweets$sentiment = json_data_frame$Sentiment
tweets$sentimentScore.mixed = json_data_frame$SentimentScore$Mixed
tweets$sentimentScore.negative = json_data_frame$SentimentScore$Negative
tweets$sentimentScore.neutral = json_data_frame$SentimentScore$Neutral
tweets$sentimentScore.positive = json_data_frame$SentimentScore$Positive
compare = tweets%>% select(text,sentiment)
head(compare,5)
tail(compare,5)
###########################################################################################
#another write out
write.csv(tweets,"tweets_loc_sen0315.csv",row.names=FALSE)
# You need to adjust the work environment to run this code
setwd("/Users/mingyang/Desktop/SMU/File Management & Database/7330-Term-Porject/Data Set")
library(tidyverse)
#plotting and pipes
library(ggplot2)
# text mining library
library(tidytext)
#Look at data
library(visdat)
vis_dat(tweets)
tweets = read_csv("fixed_location_tweets4.csv")
tweets = data.frame(tweets)
# Look at missing Data
library(naniar)
# Look at missing data
gg_miss_var(tweets)
########################################################################################
# Do some regex operation to get rid of urls and "\n" or "\r" new lines under text field
# Text processing - create function to remove url and new line in text
########################################################################################
#Reference: https://stackoverflow.com/questions/42460803/regex-in-r-remove-multiple-urls-from-string
clean_text = function(string){
#Remove URL
string = gsub("\\s?(f|ht)(tp)(s?)(://)([^\\.]*)[\\.|/](\\S*)","", string)
#Remove \r and \n
string = gsub("\r?\n|\r"," ",string)
return(string)
}
# Performance regex on tweets$text column
tweets$text = clean_text(tweets$text)
# Uniform Date Format
library(lubridate)
tweets$date = ymd_hms(tweets$date)
# Write just tweets comments column out to a csv
write.csv(tweets$text, "tweet_posts.csv", row.names=F)
tweet1 = read.csv("tweet_posts.csv",header = FALSE)
dim(tweet1)
tweet1 = read.csv("tweet_posts.csv",header = FALSE)
dim(tweet1)
tweets = tweet1
# Load the package required to read JSON files.
library("rjson")
library(jsonlite)
# Give the input file name to the function.
result <- stream_in(file("sentiment_0218.json"))
# Print the result.
head(result)
result %>% arrange(Line)
result = result %>% arrange(Line)
result$Sentiment
# Convert JSON file to a data frame.
json_data_frame <- as.data.frame(result)
# Observe the data
print(json_data_frame)
tail(json_data_frame,5)
dim(json_data_frame)
# Create new sentiment data frame frome extracting from json_data_frame
tweets$sentiment = json_data_frame$Sentiment
tweets$sentimentScore.mixed = json_data_frame$SentimentScore$Mixed
tweets$sentimentScore.negative = json_data_frame$SentimentScore$Negative
tweets$sentimentScore.neutral = json_data_frame$SentimentScore$Neutral
tweets$sentimentScore.positive = json_data_frame$SentimentScore$Positive
compare = tweets%>% select(text,sentiment)
tweets = read.csv("tweet_posts.csv",header = FALSE)
dim(tweets)
# You need to adjust the work environment to run this code
setwd("/Users/mingyang/Desktop/SMU/File Management & Database/7330-Term-Porject/Data Set")
library(tidyverse)
#plotting and pipes
library(ggplot2)
# text mining library
library(tidytext)
#Look at data
library(visdat)
vis_dat(tweets)
# Look at missing Data
library(naniar)
tweets = read_csv("fixed_location_tweets4.csv")
tweets = data.frame(tweets)
# Look at missing data
gg_miss_var(tweets)
# Look at TOP US STATES
tweets %>%
count(user_state,sort = TRUE) %>%
mutate(user_state = reorder(user_state, n)) %>%
na.omit()%>%
top_n(20)%>%
ggplot(aes(x=user_state,y=n))+
geom_col()+
coord_flip()+
labs(x = "Count",y="Location",title="Where Twitter users are from - unique State")
# Look at TOP WORLD CITIES
tweets %>%
count(user_city,sort = TRUE) %>%
mutate(user_city = reorder(user_city, n)) %>%
na.omit()%>%
top_n(20)%>%
ggplot(aes(x=user_city,y=n))+
geom_col()+
coord_flip()+
labs(x = "Count",y="Location",title="Where Twitter users are from - unique City")
# Look at TOP Countries
tweets %>%
count(user_country,sort = TRUE) %>%
mutate(user_country = reorder(user_country, n)) %>%
na.omit()%>%
top_n(20)%>%
ggplot(aes(x=user_country,y=n))+
geom_col()+
coord_flip()+
labs(x = "Count",y="Location",title="Where Twitter users are from - unique Country")
# Look at user_city less than 4 char - potential problem
#temp1 = tweets %>% filter(nchar(user_city)<=4&nchar(user_location)<35)%>%select(id,user_location,user_city,user_country,user_state)
#write.csv(temp1,"observe USA values.csv",row.names=FALSE)
########################################################################################
# Do some regex operation to get rid of urls and "\n" or "\r" new lines under text field
# Text processing - create function to remove url and new line in text
########################################################################################
#Reference: https://stackoverflow.com/questions/42460803/regex-in-r-remove-multiple-urls-from-string
clean_text = function(string){
#Remove URL
string = gsub("\\s?(f|ht)(tp)(s?)(://)([^\\.]*)[\\.|/](\\S*)","", string)
#Remove \r and \n
string = gsub("\r?\n|\r"," ",string)
return(string)
}
# Performance regex on tweets$text column
tweets$text = clean_text(tweets$text)
# Uniform Date Format
library(lubridate)
tweets$date = ymd_hms(tweets$date)
# Write just tweets comments column out to a csv
write.csv(tweets$text, "tweet_posts.csv", row.names=F)
tweet1 = read.csv("tweet_posts.csv",header = FALSE)
dim(tweets1)
tweets = read_csv("fixed_location_tweets4.csv")
tweets = data.frame(tweets)
# Look at missing data
gg_miss_var(tweets)
########################################################################################
# Do some regex operation to get rid of urls and "\n" or "\r" new lines under text field
# Text processing - create function to remove url and new line in text
########################################################################################
#Reference: https://stackoverflow.com/questions/42460803/regex-in-r-remove-multiple-urls-from-string
clean_text = function(string){
#Remove URL
string = gsub("\\s?(f|ht)(tp)(s?)(://)([^\\.]*)[\\.|/](\\S*)","", string)
#Remove \r and \n
string = gsub("\r?\n|\r"," ",string)
return(string)
}
# Performance regex on tweets$text column
tweets$text = clean_text(tweets$text)
tweets$date = ymd_hms(tweets$date)
tweets$sentimentScore.mixed = json_data_frame$SentimentScore$Mixed
tweets$sentimentScore.negative = json_data_frame$SentimentScore$Negative
tweets$sentimentScore.neutral = json_data_frame$SentimentScore$Neutral
tweets$sentimentScore.positive = json_data_frame$SentimentScore$Positive
compare = tweets%>% select(text,sentiment)
tweets = read_csv("fixed_location_tweets4.csv")
tweets = data.frame(tweets)
# Look at missing data
gg_miss_var(tweets)
########################################################################################
# Do some regex operation to get rid of urls and "\n" or "\r" new lines under text field
# Text processing - create function to remove url and new line in text
########################################################################################
#Reference: https://stackoverflow.com/questions/42460803/regex-in-r-remove-multiple-urls-from-string
clean_text = function(string){
#Remove URL
string = gsub("\\s?(f|ht)(tp)(s?)(://)([^\\.]*)[\\.|/](\\S*)","", string)
#Remove \r and \n
string = gsub("\r?\n|\r"," ",string)
return(string)
}
# Performance regex on tweets$text column
tweets$text = clean_text(tweets$text)
tweets$date = ymd_hms(tweets$date)
# Give the input file name to the function.
result <- stream_in(file("sentiment_0218.json"))
# Print the result.
head(result)
result = result %>% arrange(Line)
result$Sentiment
# Convert JSON file to a data frame.
json_data_frame <- as.data.frame(result)
# Observe the data
print(json_data_frame)
tail(json_data_frame,5)
dim(json_data_frame)
# Create new sentiment data frame frome extracting from json_data_frame
tweets$sentiment = json_data_frame$Sentiment
tweets$sentimentScore.mixed = json_data_frame$SentimentScore$Mixed
tweets$sentimentScore.negative = json_data_frame$SentimentScore$Negative
tweets$sentimentScore.neutral = json_data_frame$SentimentScore$Neutral
tweets$sentimentScore.positive = json_data_frame$SentimentScore$Positive
compare = tweets%>% select(text,sentiment)
head(compare,5)
tail(compare,5)
###########################################################################################
#another write out
write.csv(tweets,"tweets_loc_sen0223.csv",row.names=FALSE)
dim(tweets)
view(compare)
