plot(importance)
#Load NB libraries
library(e1071)
#select variables decided to predict Attrition
data.nb = employeeData %>% select(Attrition, OverTime, MonthlyIncome, TotalWorkingYears, YearsAtCompany, StockOptionLevel, MaritalStatus, JobLevel, YearsInCurrentRole, YearsWithCurrManager, Age, JobInvolvement, JobSatisfaction, JobRole, Department,Education, WorkLifeBalance, EnvironmentSatisfaction)
set.seed(12)
splitPercent = 0.80
trainIndex = sample(1:dim(data.nb)[1],round(splitPercent * dim(data.nb)[1]))
train.nb = data.nb[trainIndex,]
test.nb = data.nb[-trainIndex,]
model.nb = naiveBayes(Attrition~.,data=train.nb, laplace = 1)
predict.nb = predict(model.nb,test.nb)
table(predict.nb,test.nb$Attrition)
confusionMatrix(predict.nb,test.nb$Attrition)
#Load library to run stepwise regression method to choose an optimal simple model
library(MASS)
#Build the model with internel verfication
set.seed(24)
train.control <- trainControl(method = "cv", number = 10)
step.model = train(MonthlyIncome~., data=employeeData,
method="lmStepAIC",
trControl = train.control,
trace=FALSE)
#Model Accuracy
step.model$results
step.model$finalModel
summary(step.model$finalModel)
?sd
?pt
pt(0.95,49)
qt(0.95,49)
?qt
qt(0.975,473.85)
qnorm(0.975,13)
?qnorm
qnorm(0.975,0,1)
?pf
pt(1.06,1,280)
pt(5,1,280)
pf(1.06,1,280)
1- pf(1.06,1,280)
?pt
pt(2.84,14)
1-pt(2.84,14)
1-2*pt(2.84,14)
(1-pt(2.84,14))*2
qnorm(2.84,0,1)
?qnorm
qnorm(2.84)
pnorm(2.84)
1- pnorm(2.84)
2^(1.249-2.594*0.031)
pf(1.06,1,280)
qt(0.95,1000000000)
qnorm(0.95)
qt(0.95,100)
qt(0.95,30)
qt(0.025,30)
qt(0.025,30)
qnorm(0.025,30)
qnorm(0.025)
qt(0.025,100)
qt(0.025,1000000)
qt(0.025,10000000000)
qt(0.45,1000000000000)
qnorm(0.45)
qnorm(0.5)
qt(0.5,1000000000000)
qnorm(0.6)
qt(0.6,1000)
qt(0.6,100)
exp(0.1975)
9.381-8.667
0.714/9
0.079/0.619
pf(0.128,9,14)
1-pf(0.128,9,14)
1.99979-0.001126
0.0998499+0.0102828
1.99979-0.0001084
0.0998499+0.0303267
exp(1.999789181)
Exp(-0.001083732)
exp(-0.001083732)
exp(0.03032665)
231-7
?qt
qt(0.995,224)
0.200127253+0.00013768*2.598
0.200127253-0.00013768*2.598
qt(0.975,224)
1.998664+0.1101327*9.4+0.2001273*2
exp(3.424166)
1.998664+0.1101327*9.4+0.2001273*3
exp(3.634293)
exp(3.627926)
exp(3.640658)
0.010282784-0.030326651
(4+9-8.2)*10^08
sqrt(4.8*10^(-8))
qt(0.975,224)
-0.02004387+1.97*0.000219
-0.02004387-1.97*0.000219
0.714/5
9.381-9.667
9.381-8.667
0.1428/0.619
1-pf(0.231,5,14)
df = c(1,2,3,4,5)
scale(df)
# You need to adjust the work environment to run this code
setwd("/Users/mingyang/Desktop/SMU/File Management & Database/7330-Term-Porject/Data Set")
##########################################################################################################
# Load in fixed_city_name_tweets2.csv to continue...
read_csv("fixed_city_name_tweets2.csv")
# Resources started using:
# https://www.earthdatascience.org/courses/earth-analytics/get-data-using-apis/use-twitter-api-r/
library(tidyverse)
# text mining library
library(tidytext)
#plotting and pipes
library(ggplot2)
#Look at data
library(visdat)
# Look at missing Data
library(naniar)
library(GGally)
##########################################################################################################
# Load in fixed_city_name_tweets2.csv to continue...
read_csv("fixed_city_name_tweets2.csv")
##########################################################################################################
# Load in fixed_city_name_tweets2.csv to continue...
tweets = read_csv("fixed_city_name_tweets2.csv")
# Look at user_city less than 4 char
tweets %>% filter(nchar(user_city)<=4&nchar(user_location)<35)%>%select(id,user_location,user_city,user_country)
# Look at user_city less than 4 char
tweets %>% filter(nchar(user_city)<=4&nchar(user_location)<35 & grepl("AMERICA",user_location))%>%select(id,user_location,user_city,user_country)
# Fixing user_city = ICA
for(i in 1:tweetslength){
if(tweets[i,17]=="ICA"&(!is.na(tweets[i,17]))){
if(grepl("NORTH AMERICA",tweets[i,3])){
print(paste("fixing tweet",i))
tweets[i,17]=as.character(NA)
tweets[i,18]=as.character(NA)
print(paste("fixed",tweets[i,3]))
}
else if(grepl("AMERICA",tweets[i,3])){
print(paste("fixing tweet",i))
tweets[i,17]=as.character(NA)
tweets[i,18]="UNITED STATES"
print(paste("fixed",tweets[i,3],tweets[i,18]))
}
}
}
citylength = dim(world.cities)[1] #26569
tweetslength = dim(tweets)[1] #4139
# Fixing user_city = ICA
for(i in 1:tweetslength){
if(tweets[i,17]=="ICA"&(!is.na(tweets[i,17]))){
if(grepl("NORTH AMERICA",tweets[i,3])){
print(paste("fixing tweet",i))
tweets[i,17]=as.character(NA)
tweets[i,18]=as.character(NA)
print(paste("fixed",tweets[i,3]))
}
else if(grepl("AMERICA",tweets[i,3])){
print(paste("fixing tweet",i))
tweets[i,17]=as.character(NA)
tweets[i,18]="UNITED STATES"
print(paste("fixed",tweets[i,3],tweets[i,18]))
}
}
}
# Look at user_city less than 4 char
tweets %>% filter(nchar(user_city)<=4&nchar(user_location)<35 & grepl("AMERICA",user_location))%>%select(id,user_location,user_city,user_country)
# Look at user_city less than 4 char
tweets %>% filter(nchar(user_city)<=4&nchar(user_location)<35)%>%select(id,user_location,user_city,user_country)
# Fixing LA BELLE PROVINCE
for(i in 1:tweetslength){
if(tweets[i,17]=="BELL"&(!is.na(tweets[i,17]))){
if(grepl("LA BELLE PROVINCE",tweets[i,3])){
print(paste("fixing tweet",i))
tweets[i,17]=as.character(NA)
tweets[i,18]="CANADA"
print(paste("fixed",tweets[i,3]))
}
}
}
# Look at user_city less than 4 char
tweets %>% filter(nchar(user_city)<=4&nchar(user_location)<35)%>%select(id,user_location,user_city,user_country)
tweets %>% filter(grepl("USA",user_location))%>%select(id,user_location,user_city,user_country)
temp1 = tweets %>% filter(grepl("USA",user_location))%>%select(id,user_location,user_city,user_country)
write.csv(temp1,"observe NA values",row.names=FALSE)
write.csv(temp1,"observe NA values.csv",row.names=FALSE)
# Decide to fix USA after fixing STATES name first
usa.states = read_csv("usa_states.csv")
usa.states = toupper(usa.states)
str(usa.states)
usa.states = data.frame(usa.states)
str(usa.states)
tweets %>% filter(grepl("WASHINGTON",user_location))%>%select(id,user_location,user_city,user_country)
head(usa.states)
# Decide to fix USA after fixing STATES name first
usa.states = read_csv("usa_states.csv")
head(usa.states)
usa.states = data.frame(usa.states)
head(usa.states)
usa.states = toupper(usa.states)
head(usa.states)
# Decide to fix USA after fixing STATES name first
usa.states = read_csv("usa_states.csv")
usa.states = data.frame(usa.states)
usa.states$State = toupperusa.states$State)
usa.states$State = toupper(usa.states$State)
usa.states = usa.states[,-2]
head(usa.states)
usstatelength = dim(usa.states)[1]
usa.states[1,1]
# Finished cleaning usa states
tweets$user_state = as.character(NA)
str(tweets)
usstatelength
# Go through tweets for all states
for(i in 1:tweetslength){
for(j in 1:usstatelength){
if(grepl(usa.states[j,1],tweets[i,3])&is.na(tweets[i,18])){
print(paste("fixing tweet",i))
tweets[i,18]="UNITED STATES"
tweets[i,19]=usa.states[j,1]
print(paste("fixed",tweets[i,3],tweets[i,19],tweets[i,18]))
break
}
else if(grepl(usa.states[j,1],tweets[i,3])&tweets[i,18]!="UNITED STATES"){
print(paste("fixing tweet",i))
tweets[i,17]=as.character(NA)
tweets[i,18]="UNITED STATES"
tweets[i,19]=usa.states[j,1]
print(paste("fixed",tweets[i,3],tweets[i,19],tweets[i,18]))
break
}
}
}
tweets %>% filter(grepl("USA",user_location))%>%select(id,user_location,user_city,user_country)
temp1 = tweets %>% filter(grepl("USA",user_location))%>%select(id,user_location,user_city,user_country)
write.csv(temp1,"observe NA values.csv",row.names=FALSE)
write.csv(temp1,"observe USA values.csv",row.names=FALSE)
temp1 = tweets %>% filter(grepl("USA",user_location))%>%select(id,user_location,user_city,user_country,user_state)
write.csv(temp1,"observe USA values.csv",row.names=FALSE)
# write another loop to fix city name
for(i in 1:tweetslength){
for(j in 1:usstatelength){
if(grepl(usa.states[j,1],tweets[i,3])&tweets[i,18]=="UNITED STATES"&usa.states[j,1]!=tweets[i,17]){
print(paste("fixing tweet",i))
tweets[i,17]=as.character(NA)
tweets[i,18]="UNITED STATES"
tweets[i,19]=usa.states[j,1]
print(paste("fixed",tweets[i,3],tweets[i,19],tweets[i,18]))
break
}
else if(grepl(usa.states[j,1],tweets[i,3])&tweets[i,18]=="UNITED STATES"&usa.states[j,1]==tweets[i,17]){
print(paste("fixing tweet",i))
tweets[i,19]=usa.states[j,1]
print(paste("fixed",tweets[i,3],tweets[i,19]))
break
}
}
}
# write another loop to fix city name
for(i in 1:tweetslength){
for(j in 1:usstatelength){
if(grepl(usa.states[j,1],tweets[i,3])&(!is.na(tweets[i,18]))&tweets[i,18]=="UNITED STATES"){
if((!is.na(tweets[i,17]))&usa.states[j,1]!=tweets[i,17]){
print(paste("fixing tweet",i))
tweets[i,17]=as.character(NA)
tweets[i,18]="UNITED STATES"
tweets[i,19]=usa.states[j,1]
print(paste("fixed",tweets[i,3],tweets[i,19],tweets[i,18]))
break
}
else if((!is.na(tweets[i,17]))&usa.states[j,1]==tweets[i,17]){
print(paste("fixing tweet",i))
tweets[i,19]=usa.states[j,1]
print(paste("fixed",tweets[i,3],tweets[i,19]))
break
}
}
}
}
temp1 = tweets %>% filter(grepl("USA",user_location))%>%select(id,user_location,user_city,user_country,user_state)
write.csv(temp1,"observe USA values.csv",row.names=FALSE)
# Fixing WEST VIRGINIA, MICHIGAN
tweets %>% filter(grepl("WEST VIRGINIA",user_location))%>%select(id,user_location,user_city,user_country,user_state)
for(i in 1:tweetslength){
if((!is.na(tweets[i,3]))&grepl("WEST VIRGINIA",tweets[i,3])){
if(grepl("CHARLESTON",tweets[i,3])){
print(paste("fixing tweet",i))
tweets[i,17]="CHARLESTON"
tweets[i,19]="WEST VIRGINIA"
print(paste("fixed",tweets[i,3],tweets[i,17],tweets[i,19]))
}
else{
print(paste("fixing tweet",i))
tweets[i,17]=as.character(NA)
tweets[i,19]="WEST VIRGINIA"
print(paste("fixed",tweets[i,3],tweets[i,19]))
}
}
}
# Fixing WEST VIRGINIA, MICHIGAN
tweets %>% filter(grepl("WEST VIRGINIA",user_location))%>%select(id,user_location,user_city,user_country,user_state)
# Look at MICHIGAN
tweets %>% filter(grepl("MICHIGAN",user_location))%>%select(id,user_location,user_city,user_country,user_state)
# Fixing MICHIGAN
for(i in 1:tweetslength){
if((!is.na(tweets[i,3]))&grepl("MICHIGAN",tweets[i,3])){
if(grepl("TROY",tweets[i,3])){
print(paste("fixing tweet",i))
tweets[i,17]="TROY"
tweets[i,19]="MICHIGAN"
print(paste("fixed",tweets[i,3],tweets[i,17],tweets[i,19]))
}
else if(grepl("CANTON",tweets[i,3])){
print(paste("fixing tweet",i))
tweets[i,17]="CANTON"
tweets[i,19]="MICHIGAN"
print(paste("fixed",tweets[i,3],tweets[i,17],tweets[i,19]))
}
else {
print(paste("fixing tweet",i))
tweets[i,19]="MICHIGAN"
print(paste("fixed",tweets[i,3],tweets[i,19]))
}
}
}
temp1 = tweets %>% filter(grepl("USA",user_location))%>%select(id,user_location,user_city,user_country,user_state)
write.csv(temp1,"observe USA values.csv",row.names=FALSE)
temp1 = tweets %>% filter(grepl("USA",user_location)&user_location!="UNITED STATES")
%>%select(id,user_location,user_city,user_country,user_state)
temp1 = tweets %>% filter(grepl("USA",user_location)&user_location!="UNITED STATES")%>%
select(id,user_location,user_city,user_country,user_state)
write.csv(temp1,"observe USA values.csv",row.names=FALSE)
temp1 = tweets %>% filter(grepl("USA",user_location)&user_country!="UNITED STATES")%>%
select(id,user_location,user_city,user_country,user_state)
write.csv(temp1,"observe USA values.csv",row.names=FALSE)
# Fixing remaining USA
for(i in 1:tweetslength){
if((!is.na(tweets[i,3]))&grepl("USA",tweets[i,3])&tweets[i,18]!="UNITED STATES"){
if((!is.na(tweets[i,17]))&tweets[i,17]=="WE"){
print(paste("fixing tweet",i))
tweets[i,17]=as.character(NA)
tweets[i,18]="UNITED STATES"
tweets[i,19]=as.character(NA)
print(paste("fixed",tweets[i,3],tweets[i,18]))
}
else if((!is.na(tweets[i,17]))&tweets[i,17]=="METRO"){
print(paste("fixing tweet",i))
tweets[i,17]="WASHINGTON"
tweets[i,18]="UNITED STATES"
tweets[i,19]=as.character(NA)
print(paste("fixed",tweets[i,3],tweets[i,17],tweets[i,18]))
}
}
}
# Fixing remaining USA
for(i in 1:tweetslength){
if((!is.na(tweets[i,3]))&(!is.na(tweets[i,18]))&grepl("USA",tweets[i,3])&tweets[i,18]!="UNITED STATES"){
if((!is.na(tweets[i,17]))&tweets[i,17]=="WE"){
print(paste("fixing tweet",i))
tweets[i,17]=as.character(NA)
tweets[i,18]="UNITED STATES"
tweets[i,19]=as.character(NA)
print(paste("fixed",tweets[i,3],tweets[i,18]))
}
else if((!is.na(tweets[i,17]))&tweets[i,17]=="METRO"){
print(paste("fixing tweet",i))
tweets[i,17]="WASHINGTON"
tweets[i,18]="UNITED STATES"
tweets[i,19]=as.character(NA)
print(paste("fixed",tweets[i,3],tweets[i,17],tweets[i,18]))
}
}
}
temp1 = tweets %>% filter(grepl("USA",user_location)&user_country!="UNITED STATES")%>%
select(id,user_location,user_city,user_country,user_state)
write.csv(temp1,"observe USA values.csv",row.names=FALSE)
temp1 = tweets %>% filter(grepl("US",user_location)&!grepl("USA",user_location)&user_country!="UNITED STATES")%>%
select(id,user_location,user_city,user_country,user_state)
write.csv(temp1,"observe USA values.csv",row.names=FALSE)
temp1 = tweets %>% filter(grepl("NYC",user_location)&user_country!="UNITED STATES")%>%
select(id,user_location,user_city,user_country,user_state)
write.csv(temp1,"observe USA values.csv",row.names=FALSE)
temp1 = tweets %>% filter(grepl("LA",user_location)&user_country!="UNITED STATES")%>%
select(id,user_location,user_city,user_country,user_state)
write.csv(temp1,"observe USA values.csv",row.names=FALSE)
###########################################################################################3
#another write out
write.csv(tweets,"fixed_location_tweets3.csv",row.names=FALSE)
gg_miss_var(tweets)
###########################################################################################
# Look at user_city less than 4 char
tweets %>% filter(nchar(user_city)<=4&nchar(user_location)<35)%>%select(id,user_location,user_city,user_country)
###########################################################################################
# Look at user_city less than 4 char
tweets %>% filter(nchar(user_city)<=4&nchar(user_location)<35)%>%select(id,user_location,user_city,user_country,user_state)
###########################################################################################
# Look at user_city less than 4 char
temp1 = tweets %>% filter(nchar(user_city)<=4&nchar(user_location)<35)%>%select(id,user_location,user_city,user_country,user_state)
write.csv(temp1,"observe USA values.csv",row.names=FALSE)
tweets %>%filter(grepl("CRAMBURY, NJ",user_location))%>%select(id,user_location,user_city,user_country,user_state)
tweets %>%filter(grepl("CRANBURY, NJ",user_location))%>%select(id,user_location,user_city,user_country,user_state)
# Fixing CRANBURY, NJ
for(i in 1:tweetslength){
if((!is.na(tweets[i,3]))&grepl("CRANBURY, NJ",tweets[i,3])){
print(paste("fixing tweet",i))
tweets[i,17]="CRANBURY"
tweets[i,18]="UNITED STATES"
tweets[i,19]="NEW JERSEY"
print(paste("fixed",tweets[i,3],tweets[i,17],tweets[i,19],tweets[i,18]))
break
}
}
tweets %>%filter(grepl("CRANBURY, NJ",user_location))%>%select(id,user_location,user_city,user_country,user_state)
###########################################################################################
#another write out
write.csv(tweets,"fixed_location_tweets3.csv",row.names=FALSE)
###########################################################################################
# Look at US STATES
tweets %>%
count(user_state,sort = TRUE) %>%
mutate(user_state = reorder(user_state, n)) %>%
na.omit()%>%
top_n(20)%>%
ggplot(aes(x=user_state,y=n))+
geom_col()+
coord_flip()+
labs(x = "Count",y="Location",title="Where Twitter users are from - unique State")
# Look at TOP WORLD CITIES
tweets %>%
count(user_city,sort = TRUE) %>%
mutate(user_city = reorder(user_city, n)) %>%
na.omit()%>%
top_n(20)%>%
ggplot(aes(x=user_city,y=n))+
geom_col()+
coord_flip()+
labs(x = "Count",y="Location",title="Where Twitter users are from - unique City")
# Look at NYC where city is NA
tweets %>% filter(grepl("NYC",user_location)&is.na(user_city))%>%select(id,user_location,user_city,user_country,user_state)
# Fixing NYC where user_city is NA
for(i in 1:tweetslength){
if((!is.na(tweets[i,3]))&grepl("NYC",tweets[i,3])&is.na(tweets[i,17])){
print(paste("fixing tweet",i))
tweets[i,17]="NEW YORK"
tweets[i,18]="UNITED STATES"
tweets[i,19]="NEW YORK"
print(paste("fixed",tweets[i,3],tweets[i,17],tweets[i,19],tweets[i,18]))
}
}
# Look at NYC where city is NA
tweets %>% filter(grepl("NYC",user_location)&is.na(user_city))%>%select(id,user_location,user_city,user_country,user_state)
###########################################################################################
#another write out
write.csv(tweets,"fixed_location_tweets3.csv",row.names=FALSE)
###########################################################################################
# Look at TOP US STATES
tweets %>%
count(user_state,sort = TRUE) %>%
mutate(user_state = reorder(user_state, n)) %>%
na.omit()%>%
top_n(20)%>%
ggplot(aes(x=user_state,y=n))+
geom_col()+
coord_flip()+
labs(x = "Count",y="Location",title="Where Twitter users are from - unique State")
# Look at TOP WORLD CITIES
tweets %>%
count(user_city,sort = TRUE) %>%
mutate(user_city = reorder(user_city, n)) %>%
na.omit()%>%
top_n(20)%>%
ggplot(aes(x=user_city,y=n))+
geom_col()+
coord_flip()+
labs(x = "Count",y="Location",title="Where Twitter users are from - unique City")
###########################################################################################
#another write out
write.csv(tweets,"fixed_location_tweets3.csv",row.names=FALSE)
# Look at TOP Countries
tweets %>%
count(user_country,sort = TRUE) %>%
mutate(user_country = reorder(user_country, n)) %>%
na.omit()%>%
top_n(20)%>%
ggplot(aes(x=user_country,y=n))+
geom_col()+
coord_flip()+
labs(x = "Count",y="Location",title="Where Twitter users are from - unique Country")
tweets = read_csv("fixed_location_tweets3.csv")
# Look at TOP US STATES
tweets %>%
count(user_state,sort = TRUE) %>%
mutate(user_state = reorder(user_state, n)) %>%
na.omit()%>%
top_n(20)%>%
ggplot(aes(x=user_state,y=n))+
geom_col()+
coord_flip()+
labs(x = "Count",y="Location",title="Where Twitter users are from - unique State")
gg_miss_var(tweets)
tweets = read_csv("fixed_location_tweets3.csv")
tweets = data.frame(tweets)
str(tweets)
