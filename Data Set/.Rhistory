names(train)<-paste("X",1:20,sep="")
train$Response<-rep(c("Yes","No"),each=30)
train$Response<-factor(train$Response)
#Creating a test set
muYes<-c(10,10)
muNo<-c(8,8)
Sigma<-matrix(c(1,.8,.8,1),2,2,byrow=T)
nY<-500
nN<-500
dataYes<-rmvnorm(nY,muYes,Sigma)
dataNo<- rmvnorm(nN,muNo,Sigma)
test<-rbind(dataYes,dataNo)
test<-data.frame(test)
for (i in 3:20){
test<-cbind(test,rnorm(nY+nN))
}
names(test)<-paste("X",1:20,sep="")
test$Response<-rep(c("Yes","No"),each=500)
test$Response<-factor(test$Response)
mylda<-lda(Response~X1+X2+X3+X4+X5+X6+X7+X8+X9+X10,data=train)
pred<-predict(mylda,newdata=test)$class  #Predictions can come in many forms, the class form provides the categorical level of your response.
Truth<-test$Response
x<-table(pred,Truth) # Creating a confusion matrix
x
#Missclassification Error
ME<-(x[2,1]+x[1,2])/1000
ME
#Calculating overall accuracy
1-ME
mylda<-lda(Response~.,data=train)
mylda
mylda<-lda(Response~X1+X2+X3+X4+X5+X6+X7+X8+X9+X10,data=train)
mylda
.5+1.96*sqrt(.5*.5/1000)
.5+1.96*sqrt(.5*.5/1000)
0.516129-0.311475
19*30/42*32
19*30/42/32
32*42/19/30
sqrt(0.516129*(1-0.516129)/62+0.311475*(1-0.311475)/61)
0.08685489*1.96
0.204654-0.1702356
0.204654+0.1702356
log(2.36)
sqrt(1/32+1/30+1/19+1/42)
0.86-1.96*0.3755
0.86+1.96*0.3755
exp(0.124)
exp(1.596)
?pnorm
(32+19)/(62+61)
sqrt(0.4146*(1-0.4146)/61+0.4146*(1-0.4146)/62)
0.204654/0.08884496
pnorm(2.3035,0,1)
1-pnorm(2.3035,0,1)
(1-pnorm(2.3035,0,1))*2
0.86/0.3755
(1-pnorm(2.29,0,1))*2
(76/411)/(105/407)
76*302/105/335
(302/407)/(335/411)
(335/411)/(302/407)
(335/76)/(302/105)
2499.96+1325.56+184.61
4010.13/9208.65
0.56688599*8345.27
# You need to adjust the work environment to run this code
setwd("/Users/mingyang/Desktop/SMU/File Management & Database/7330-Term-Porject/Data Set")
library(tidyverse)
# text mining library
library(tidytext)
# Look at missing Data
library(naniar)
#plotting and pipes
library(ggplot2)
tweets = read_csv("tweets_loc_sen0223.csv")
#Look at data
library(visdat)
tweets = data.frame(tweets)
# Look at missing data
gg_miss_var(tweets)
# Look at structure of tweets
str(tweets)
# Look at TOP US STATES
tweets %>%
count(user_state,sort = TRUE) %>%
mutate(user_state = reorder(user_state, n)) %>%
na.omit()%>%
top_n(20)%>%
ggplot(aes(x=user_state,y=n))+
geom_col()+
coord_flip()+
labs(x = "Count",y="Location",title="Where Twitter users are from - unique State")
# Look at TOP WORLD CITIES
tweets %>%
count(user_city,sort = TRUE) %>%
mutate(user_city = reorder(user_city, n)) %>%
na.omit()%>%
top_n(20)%>%
ggplot(aes(x=user_city,y=n))+
geom_col()+
coord_flip()+
labs(x = "Count",y="Location",title="Where Twitter users are from - unique City")
# Look at TOP Countries
tweets %>%
count(user_country,sort = TRUE) %>%
mutate(user_country = reorder(user_country, n)) %>%
na.omit()%>%
top_n(20)%>%
ggplot(aes(x=user_country,y=n))+
geom_col()+
coord_flip()+
labs(x = "Count",y="Location",title="Where Twitter users are from - unique Country")
# Make Sentiment into factor
tweets$sentiment = as.factor(tweets$sentiment)
# Look at USA sentiment
usa_sentiment = tweets %>% filter(user_country=="UNITED STATES")%>%select(user_country,sentiment)
ggplot(data = usa_sentiment) +
geom_bar(mapping = aes(x = sentiment, y = stat(prop), group = 1))+
ggtitle("United States Pfizer Vaccine Sentiment")
# Look at Sentiment in US by States
tweets %>% filter(!is.na(user_state)) %>%
select(user_state,sentiment) %>%
ggplot(aes(user_state, ..count..))+
geom_bar(aes(fill=sentiment))+coord_flip()+ggtitle("Sentiment across all US States")
# Look at Sentiment in the world by countries
tweets %>% filter(!is.na(user_country)) %>%
filter(user_country=="UNITED STATES"|user_country=="UNITED KINGDOM"|
user_country=="CANADA"|user_country=="INDIA"|
user_country=="UNITED ARAB EMIRATES"|user_country=="IRELAND"|
user_country=="FRANCE"|user_country=="GERMANY")%>%
mutate(user_country = fct_relevel(user_country,
"GERMANY", "FRANCE", "IRELAND",
"UNITED ARAB EMIRATES", "INDIA", "CANADA",
"UNITED KINGDOM", "UNITED STATES")) %>%
select(user_country,sentiment) %>%
ggplot(aes(user_country, ..count..))+
geom_bar(aes(fill=sentiment))+coord_flip()+
ggtitle("Sentiment across top 8 country by tweets")
# Look at Sentiment in the US by states - top 8 states tweets
tweets %>% filter(!is.na(user_state)) %>%
filter(user_state=="CALIFORNIA"|user_state=="NEW YORK"|
user_state=="TEXAS"|user_state=="FLORIDA"|
user_state=="WASHINGTON"|user_state=="ILLINOIS"|
user_state=="GEORGIA"|user_state=="MICHIGAN")%>%
mutate(user_state = fct_relevel(user_state,
"MICHIGAN", "GEORGIA", "ILLINOIS",
"WASHINGTON", "FLORIDA", "TEXAS",
"NEW YORK", "CALIFORNIA")) %>%
select(user_state,sentiment) %>%
ggplot(aes(user_state, ..count..))+
geom_bar(aes(fill=sentiment))+coord_flip()+
ggtitle("Sentiment across top 8 US states by tweets")
# Look at Sentiment for top 8 cities by tweets
tweets %>% filter(!is.na(user_city)) %>%
filter(user_city=="LONDON"|user_city=="NEW YORK"|
user_city=="DUBAI"|user_city=="NEW DELHI"|
user_city=="TORONTO"|user_city=="MUMBAI"|
user_city=="GLASGOW"|user_city=="CALIFORNIA")%>%
mutate(user_city = fct_relevel(user_city,
"CALIFORNIA", "GLASGOW", "MUMBAI",
"TORONTO", "NEW DELHI", "DUBAI",
"NEW YORK", "LONDON")) %>%
select(user_city,sentiment) %>%
ggplot(aes(user_city, ..count..))+
geom_bar(aes(fill=sentiment))+coord_flip()+
ggtitle("Sentiment across top 8 world cities by tweets")
library(stringr)
library(tm)
library(SnowballC)
library(wordcloud)
library(RColorBrewer)
#text =  "['coronavirus', 'SputnikV', 'AstraZeneca', 'PfizerBioNTech', 'Moderna', 'Covid_19']"
hashtags = str_extract_all(tweets$hashtags,"\'.*\'")
hashtags = unlist(hashtags)
hashtags = hashtags %>% na.omit()
# get rid of single quotes
hashtags = gsub("'", '', hashtags)
# create a corpus
hashtags_corpus = Corpus(VectorSource(hashtags))
# Convert the text to lower case
hashtags_corpus <- tm_map(hashtags_corpus, content_transformer(tolower))
# remove custom stop words
custom.stop.words = c("pfizerbiontech","covidvaccine","vaccine","covid19")
hashtags_corpus = tm_map(hashtags_corpus,removeWords,custom.stop.words)
#inspect(hashtags_corpus)
# create document term matrix applying some transformations
tdm = TermDocumentMatrix(hashtags_corpus,
control = list(removePunctuation = TRUE))
# define tdm as matrix
m = as.matrix(tdm)
# get word counts in decreasing order
word_freqs = sort(rowSums(m), decreasing=TRUE)
# create a data frame with words and their frequencies
dm = data.frame(word=names(word_freqs), freq=word_freqs)
# plot wordcloud
set.seed(100)
wordcloud(dm$word, dm$freq, random.order=FALSE, random.color=TRUE, colors=brewer.pal(8, "Dark2"))
pos.tweets = tweets %>% filter(sentiment=="POSITIVE")
hashtags = str_extract_all(pos.tweets$hashtags,"\'.*\'")
hashtags = unlist(hashtags)
hashtags = hashtags %>% na.omit()
# get rid of single quotes
hashtags = gsub("'", '', hashtags)
# create a corpus
hashtags_corpus = Corpus(VectorSource(hashtags))
# Convert the text to lower case
hashtags_corpus <- tm_map(hashtags_corpus, content_transformer(tolower))
# remove custom stop words
custom.stop.words = c("pfizerbiontech","covidvaccine","vaccine","covid19")
hashtags_corpus = tm_map(hashtags_corpus,removeWords,custom.stop.words)
#inspect(hashtags_corpus)
# create document term matrix applying some transformations
tdm = TermDocumentMatrix(hashtags_corpus,
control = list(removePunctuation = TRUE))
# define tdm as matrix
m = as.matrix(tdm)
# get word counts in decreasing order
word_freqs = sort(rowSums(m), decreasing=TRUE)
# create a data frame with words and their frequencies
dm = data.frame(word=names(word_freqs), freq=word_freqs)
# plot wordcloud
set.seed(105)
wordcloud(dm$word, dm$freq, random.order=FALSE, random.color=TRUE, colors=brewer.pal(8, "Dark2"))
neg.tweets = tweets %>% filter(sentiment=="NEGATIVE")
hashtags = str_extract_all(neg.tweets$hashtags,"\'.*\'")
hashtags = unlist(hashtags)
hashtags = hashtags %>% na.omit()
# get rid of single quotes
hashtags = gsub("'", '', hashtags)
# create a corpus
hashtags_corpus = Corpus(VectorSource(hashtags))
# Convert the text to lower case
hashtags_corpus <- tm_map(hashtags_corpus, content_transformer(tolower))
# remove custom stop words
custom.stop.words = c("pfizerbiontech","covidvaccine","vaccine","covid19")
hashtags_corpus = tm_map(hashtags_corpus,removeWords,custom.stop.words)
# create document term matrix applying some transformations
tdm = TermDocumentMatrix(hashtags_corpus,
control = list(removePunctuation = TRUE))
# define tdm as matrix
m = as.matrix(tdm)
# get word counts in decreasing order
word_freqs = sort(rowSums(m), decreasing=TRUE)
# create a data frame with words and their frequencies
dm = data.frame(word=names(word_freqs), freq=word_freqs)
# plot wordcloud
set.seed(103)
wordcloud(dm$word, dm$freq, random.order=FALSE, random.color=TRUE, colors=brewer.pal(8, "Dark2"))
neutral.tweets = tweets %>% filter(sentiment=="NEUTRAL")
hashtags = str_extract_all(neutral.tweets$hashtags,"\'.*\'")
hashtags = unlist(hashtags)
hashtags = hashtags %>% na.omit()
# get rid of single quotes
hashtags = gsub("'", '', hashtags)
# create a corpus
hashtags_corpus = Corpus(VectorSource(hashtags))
# Convert the text to lower case
hashtags_corpus <- tm_map(hashtags_corpus, content_transformer(tolower))
# remove custom stop words
custom.stop.words = c("pfizerbiontech","covidvaccine","vaccine","covid19")
hashtags_corpus = tm_map(hashtags_corpus,removeWords,custom.stop.words)
# create document term matrix applying some transformations
tdm = TermDocumentMatrix(hashtags_corpus,
control = list(removePunctuation = TRUE))
# define tdm as matrix
m = as.matrix(tdm)
# get word counts in decreasing order
word_freqs = sort(rowSums(m), decreasing=TRUE)
# create a data frame with words and their frequencies
dm = data.frame(word=names(word_freqs), freq=word_freqs)
# plot wordcloud
set.seed(103)
wordcloud(dm$word, dm$freq, random.order=FALSE, random.color=TRUE, colors=brewer.pal(8, "Dark2"))
mixed.tweets = tweets %>% filter(sentiment=="MIXED")
hashtags = str_extract_all(mixed.tweets$hashtags,"\'.*\'")
hashtags = unlist(hashtags)
hashtags = hashtags %>% na.omit()
# get rid of single quotes
hashtags = gsub("'", '', hashtags)
# create a corpus
hashtags_corpus = Corpus(VectorSource(hashtags))
# Convert the text to lower case
hashtags_corpus <- tm_map(hashtags_corpus, content_transformer(tolower))
# remove custom stop words
custom.stop.words = c("pfizerbiontech","covidvaccine","vaccine","covid19")
hashtags_corpus = tm_map(hashtags_corpus,removeWords,custom.stop.words)
# create document term matrix applying some transformations
tdm = TermDocumentMatrix(hashtags_corpus,
control = list(removePunctuation = TRUE))
# define tdm as matrix
m = as.matrix(tdm)
# get word counts in decreasing order
word_freqs = sort(rowSums(m), decreasing=TRUE)
# create a data frame with words and their frequencies
dm = data.frame(word=names(word_freqs), freq=word_freqs)
# plot wordcloud
set.seed(103)
wordcloud(dm$word, dm$freq, random.order=FALSE, random.color=TRUE, colors=brewer.pal(8, "Dark2"))
#tweets = read_csv("tweets_loc_sen0223.csv")
#tweets = data.frame(tweets)
tweets = read_csv("tweets_loc_sen0315.csv")
tweets = data.frame(tweets)
# Look at missing data
gg_miss_var(tweets)
library(stringr)
library(tm)
library(SnowballC)
library(wordcloud)
library(RColorBrewer)
#text =  "['coronavirus', 'SputnikV', 'AstraZeneca', 'PfizerBioNTech', 'Moderna', 'Covid_19']"
hashtags = str_extract_all(tweets$hashtags,"\'.*\'")
hashtags = unlist(hashtags)
hashtags = hashtags %>% na.omit()
# get rid of single quotes
hashtags = gsub("'", '', hashtags)
# create a corpus
hashtags_corpus = Corpus(VectorSource(hashtags))
# Convert the text to lower case
hashtags_corpus <- tm_map(hashtags_corpus, content_transformer(tolower))
# remove custom stop words
custom.stop.words = c("pfizerbiontech","covidvaccine","vaccine","covid19")
hashtags_corpus = tm_map(hashtags_corpus,removeWords,custom.stop.words)
#inspect(hashtags_corpus)
# create document term matrix applying some transformations
tdm = TermDocumentMatrix(hashtags_corpus,
control = list(removePunctuation = TRUE))
# define tdm as matrix
m = as.matrix(tdm)
# get word counts in decreasing order
word_freqs = sort(rowSums(m), decreasing=TRUE)
# create a data frame with words and their frequencies
dm = data.frame(word=names(word_freqs), freq=word_freqs)
# plot wordcloud
set.seed(100)
wordcloud(dm$word, dm$freq, random.order=FALSE, random.color=TRUE, colors=brewer.pal(8, "Dark2"))
pos.tweets = tweets %>% filter(sentiment=="POSITIVE")
hashtags = str_extract_all(pos.tweets$hashtags,"\'.*\'")
hashtags = unlist(hashtags)
hashtags = hashtags %>% na.omit()
# get rid of single quotes
hashtags = gsub("'", '', hashtags)
# create a corpus
hashtags_corpus = Corpus(VectorSource(hashtags))
# Convert the text to lower case
hashtags_corpus <- tm_map(hashtags_corpus, content_transformer(tolower))
# remove custom stop words
custom.stop.words = c("pfizerbiontech","covidvaccine","vaccine","covid19")
hashtags_corpus = tm_map(hashtags_corpus,removeWords,custom.stop.words)
#inspect(hashtags_corpus)
# create document term matrix applying some transformations
tdm = TermDocumentMatrix(hashtags_corpus,
control = list(removePunctuation = TRUE))
# define tdm as matrix
m = as.matrix(tdm)
# get word counts in decreasing order
word_freqs = sort(rowSums(m), decreasing=TRUE)
# create a data frame with words and their frequencies
dm = data.frame(word=names(word_freqs), freq=word_freqs)
# plot wordcloud
set.seed(105)
wordcloud(dm$word, dm$freq, random.order=FALSE, random.color=TRUE, colors=brewer.pal(8, "Dark2"))
neg.tweets = tweets %>% filter(sentiment=="NEGATIVE")
hashtags = str_extract_all(neg.tweets$hashtags,"\'.*\'")
hashtags = unlist(hashtags)
hashtags = hashtags %>% na.omit()
# get rid of single quotes
hashtags = gsub("'", '', hashtags)
# create a corpus
hashtags_corpus = Corpus(VectorSource(hashtags))
# Convert the text to lower case
hashtags_corpus <- tm_map(hashtags_corpus, content_transformer(tolower))
# remove custom stop words
custom.stop.words = c("pfizerbiontech","covidvaccine","vaccine","covid19")
hashtags_corpus = tm_map(hashtags_corpus,removeWords,custom.stop.words)
# create document term matrix applying some transformations
tdm = TermDocumentMatrix(hashtags_corpus,
control = list(removePunctuation = TRUE))
# define tdm as matrix
m = as.matrix(tdm)
# get word counts in decreasing order
word_freqs = sort(rowSums(m), decreasing=TRUE)
# create a data frame with words and their frequencies
dm = data.frame(word=names(word_freqs), freq=word_freqs)
# plot wordcloud
set.seed(103)
wordcloud(dm$word, dm$freq, random.order=FALSE, random.color=TRUE, colors=brewer.pal(8, "Dark2"))
neg.tweets = tweets %>% filter(sentiment=="NEGATIVE")
hashtags = str_extract_all(neg.tweets$hashtags,"\'.*\'")
hashtags = unlist(hashtags)
hashtags = hashtags %>% na.omit()
# get rid of single quotes
hashtags = gsub("'", '', hashtags)
# create a corpus
hashtags_corpus = Corpus(VectorSource(hashtags))
# Convert the text to lower case
hashtags_corpus <- tm_map(hashtags_corpus, content_transformer(tolower))
# remove custom stop words
custom.stop.words = c("pfizerbiontech","covidvaccine","vaccine","covid19")
hashtags_corpus = tm_map(hashtags_corpus,removeWords,custom.stop.words)
# create document term matrix applying some transformations
tdm = TermDocumentMatrix(hashtags_corpus,
control = list(removePunctuation = TRUE))
# define tdm as matrix
m = as.matrix(tdm)
# get word counts in decreasing order
word_freqs = sort(rowSums(m), decreasing=TRUE)
# create a data frame with words and their frequencies
dm = data.frame(word=names(word_freqs), freq=word_freqs)
# plot wordcloud
set.seed(104)
wordcloud(dm$word, dm$freq, random.order=FALSE, random.color=TRUE, colors=brewer.pal(8, "Dark2"))
neutral.tweets = tweets %>% filter(sentiment=="NEUTRAL")
hashtags = str_extract_all(neutral.tweets$hashtags,"\'.*\'")
hashtags = unlist(hashtags)
hashtags = hashtags %>% na.omit()
# get rid of single quotes
hashtags = gsub("'", '', hashtags)
# create a corpus
hashtags_corpus = Corpus(VectorSource(hashtags))
# Convert the text to lower case
hashtags_corpus <- tm_map(hashtags_corpus, content_transformer(tolower))
# remove custom stop words
custom.stop.words = c("pfizerbiontech","covidvaccine","vaccine","covid19")
hashtags_corpus = tm_map(hashtags_corpus,removeWords,custom.stop.words)
# create document term matrix applying some transformations
tdm = TermDocumentMatrix(hashtags_corpus,
control = list(removePunctuation = TRUE))
# define tdm as matrix
m = as.matrix(tdm)
# get word counts in decreasing order
word_freqs = sort(rowSums(m), decreasing=TRUE)
# create a data frame with words and their frequencies
dm = data.frame(word=names(word_freqs), freq=word_freqs)
# plot wordcloud
set.seed(103)
wordcloud(dm$word, dm$freq, random.order=FALSE, random.color=TRUE, colors=brewer.pal(8, "Dark2"))
mixed.tweets = tweets %>% filter(sentiment=="MIXED")
hashtags = str_extract_all(mixed.tweets$hashtags,"\'.*\'")
hashtags = unlist(hashtags)
hashtags = hashtags %>% na.omit()
# get rid of single quotes
hashtags = gsub("'", '', hashtags)
# create a corpus
hashtags_corpus = Corpus(VectorSource(hashtags))
# Convert the text to lower case
hashtags_corpus <- tm_map(hashtags_corpus, content_transformer(tolower))
# remove custom stop words
custom.stop.words = c("pfizerbiontech","covidvaccine","vaccine","covid19")
hashtags_corpus = tm_map(hashtags_corpus,removeWords,custom.stop.words)
# create document term matrix applying some transformations
tdm = TermDocumentMatrix(hashtags_corpus,
control = list(removePunctuation = TRUE))
# define tdm as matrix
m = as.matrix(tdm)
# get word counts in decreasing order
word_freqs = sort(rowSums(m), decreasing=TRUE)
# create a data frame with words and their frequencies
dm = data.frame(word=names(word_freqs), freq=word_freqs)
# plot wordcloud
set.seed(103)
wordcloud(dm$word, dm$freq, random.order=FALSE, random.color=TRUE, colors=brewer.pal(8, "Dark2"))
mixed.tweets = tweets %>% filter(sentiment=="MIXED")
hashtags = str_extract_all(mixed.tweets$hashtags,"\'.*\'")
hashtags = unlist(hashtags)
hashtags = hashtags %>% na.omit()
# get rid of single quotes
hashtags = gsub("'", '', hashtags)
# create a corpus
hashtags_corpus = Corpus(VectorSource(hashtags))
# Convert the text to lower case
hashtags_corpus <- tm_map(hashtags_corpus, content_transformer(tolower))
# remove custom stop words
custom.stop.words = c("pfizerbiontech","covidvaccine","vaccine","covid19")
hashtags_corpus = tm_map(hashtags_corpus,removeWords,custom.stop.words)
# create document term matrix applying some transformations
tdm = TermDocumentMatrix(hashtags_corpus,
control = list(removePunctuation = TRUE))
# define tdm as matrix
m = as.matrix(tdm)
# get word counts in decreasing order
word_freqs = sort(rowSums(m), decreasing=TRUE)
# create a data frame with words and their frequencies
dm = data.frame(word=names(word_freqs), freq=word_freqs)
# plot wordcloud
set.seed(104)
wordcloud(dm$word, dm$freq, random.order=FALSE, random.color=TRUE, colors=brewer.pal(8, "Dark2"))
mixed.tweets = tweets %>% filter(sentiment=="MIXED")
hashtags = str_extract_all(mixed.tweets$hashtags,"\'.*\'")
hashtags = unlist(hashtags)
hashtags = hashtags %>% na.omit()
# get rid of single quotes
hashtags = gsub("'", '', hashtags)
# create a corpus
hashtags_corpus = Corpus(VectorSource(hashtags))
# Convert the text to lower case
hashtags_corpus <- tm_map(hashtags_corpus, content_transformer(tolower))
# remove custom stop words
custom.stop.words = c("pfizerbiontech","covidvaccine","vaccine","covid19")
hashtags_corpus = tm_map(hashtags_corpus,removeWords,custom.stop.words)
# create document term matrix applying some transformations
tdm = TermDocumentMatrix(hashtags_corpus,
control = list(removePunctuation = TRUE))
# define tdm as matrix
m = as.matrix(tdm)
# get word counts in decreasing order
word_freqs = sort(rowSums(m), decreasing=TRUE)
# create a data frame with words and their frequencies
dm = data.frame(word=names(word_freqs), freq=word_freqs)
# plot wordcloud
set.seed(105)
wordcloud(dm$word, dm$freq, random.order=FALSE, random.color=TRUE, colors=brewer.pal(8, "Dark2"))
