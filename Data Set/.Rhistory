anova(model3)
confint(model3)
summary(model3)
data2$Neighborhood = relevel(data2$Neighborhood,ref="Edwards")
model3 = lm(SalePrice~GrLivArea+Neighborhood+GrLivArea*Neighborhood,data=data2)
summary(model3)
confint(model3)
# Change reference to BrkSide to get CI for slope
data2$Neighborhood = relevel(data2$Neighborhood,ref="BrkSide")
model3 = lm(SalePrice~GrLivArea+Neighborhood+GrLivArea*Neighborhood,data=data2)
summary(model3)
confint(model3)
# Change reference to NAmes to get CI for slope
data2$Neighborhood = relevel(data2$Neighborhood,ref="NAmes")
model3 = lm(SalePrice~GrLivArea+Neighborhood+GrLivArea*Neighborhood,data=data2)
summary(model3)
confint(model3)
# convert certain columns we need into factor in order to use multi regression model
cols.to.factor = c("MSSubClass","MSZoning","Street","Alley","LotShape","LandContour","Utilities",
"LotConfig","LandSlope","Neighborhood","Condition1","Condition2","BldgType",
"HouseStyle","OverallQual","OverallCond","RoofStyle","RoofMatl","Exterior1st",
"Exterior2nd","MasVnrType","ExterQual","ExterCond","Foundation","BsmtQual",
"BsmtCond","BsmtExposure","BsmtFinType1","BsmtFinType2","Heating","HeatingQC",
"CentralAir","Electrical","KitchenQual","Functional","FireplaceQu","GarageType",
"GarageFinish","GarageQual","GarageCond","PavedDrive","PoolQC","Fence","MiscFeature",
"SaleType","SaleCondition","PoolArea","MoSold","YrSold","Fireplaces","FullBath")
amesHouse[cols.to.factor] = lapply(amesHouse[cols.to.factor],factor)
#Now exploring relationships between continuous variables
amesHouse %>% dplyr::select(SalePrice,LotFrontage,LotArea,YearBuilt,YearRemodAdd,MasVnrArea,BsmtFinSF1,BsmtFinSF2)%>%
ggpairs()
#Try Logrithmic on SalePrice
amesHouse$lSalePrice = log(amesHouse$SalePrice)
#explore relationship between log SalePrice
amesHouse %>% dplyr::select(lSalePrice,LotFrontage,LotArea,YearBuilt,YearRemodAdd,MasVnrArea,BsmtFinSF1,BsmtFinSF2)%>%
ggpairs()
#Move forward with next batch of exploration
amesHouse %>% dplyr::select(SalePrice,BsmtUnfSF,TotalBsmtSF,X1stFlrSF,X2ndFlrSF,LowQualFinSF,GrLivArea) %>%
ggpairs()
amesHouse %>% dplyr::select(lSalePrice,BsmtUnfSF,TotalBsmtSF,X1stFlrSF,X2ndFlrSF,LowQualFinSF,GrLivArea) %>%
ggpairs()
#Move forward with next batch of exploration
amesHouse %>% dplyr::select(SalePrice,BsmtFullBath,BsmtHalfBath,HalfBath,BedroomAbvGr,KitchenAbvGr,
TotRmsAbvGrd)%>%
ggpairs()
amesHouse %>% dplyr::select(lSalePrice,BsmtUnfSF,TotalBsmtSF,X1stFlrSF,X2ndFlrSF,LowQualFinSF,GrLivArea) %>%
ggpairs()
#Move forward with next batch of exploration
amesHouse %>% dplyr::select(SalePrice,BsmtFullBath,BsmtHalfBath,HalfBath,BedroomAbvGr,KitchenAbvGr,
TotRmsAbvGrd)%>%
ggpairs()
amesHouse %>% dplyr::select(lSalePrice,BsmtFullBath,BsmtHalfBath,HalfBath,BedroomAbvGr,KitchenAbvGr,
TotRmsAbvGrd)%>%
ggpairs()
#The only variable seem to have good correlation with lSalePrice is FullBath,TotRmsAbvGrd.
#???If we should change BsmtFullBath,BsmtHalfBath,HalfBath,BedroomAbvGr,KitchenAbvGr to factors???
#Keep going
amesHouse %>% dplyr::select(SalePrice,GarageYrBlt,GarageCars,GarageArea) %>% ggpairs()
amesHouse %>% dplyr::select(lSalePrice,GarageYrBlt,GarageCars,GarageArea) %>% ggpairs()
#The above variables all seem to have a good linear correlation with lSalePrice
#Keep going
amesHouse %>% dplyr::select(SalePrice,WoodDeckSF,OpenPorchSF,EnclosedPorch,X3SsnPorch,ScreenPorch,MiscVal)%>%
ggpairs()
amesHouse %>% dplyr::select(lSalePrice,WoodDeckSF,OpenPorchSF,EnclosedPorch,X3SsnPorch,ScreenPorch,MiscVal)%>%
ggpairs()
#Loading libraries needed
library(tidyverse)
library(ggplot2)
#Loading in Employee data
employeeData = read.csv("/Users/mingyang/Desktop/SMU/DoingDS_Fall2020/CaseStudy2DDS/CaseStudy2-data.csv",header = TRUE)
summary(employeeData)
#See which column has only unique value
sapply(employeeData,function(col) length(unique(col)))
#Delete column that has only one unique value
to.be.deleted = which(sapply(employeeData,function(col) length(unique(col))==1))
employeeData = employeeData[,-to.be.deleted]
#Convert some values into factors
cols.to.factor = c("Attrition","BusinessTravel","Department","EducationField","EnvironmentSatisfaction",
"Gender","JobInvolvement","JobLevel","JobRole","JobSatisfaction","MaritalStatus","NumCompaniesWorked",
"OverTime","PerformanceRating","RelationshipSatisfaction","StockOptionLevel","TrainingTimesLastYear",
"WorkLifeBalance")
employeeData[cols.to.factor] = lapply(employeeData[cols.to.factor],factor)
#Load in libraries
library(lattice)
library(caret)
library(mlbench)
#prepare training scheme
control = trainControl(method="repeatedcv", number=10, repeats=3)
#train the model
model = train(Attrition~.,data=employeeData,method="lvq",preProcess="scale", trControl=control)
#estimate variable importance
importance = varImp(model,scale=FALSE)
#summarize importance
print(importance)
#plot importance
plot(importance)
#Load NB libraries
library(e1071)
#select variables decided to predict Attrition
data.nb = employeeData %>% select(Attrition, OverTime, MonthlyIncome, TotalWorkingYears, YearsAtCompany, StockOptionLevel, MaritalStatus, JobLevel, YearsInCurrentRole, YearsWithCurrManager, Age, JobInvolvement, JobSatisfaction, JobRole, Department,Education, WorkLifeBalance, EnvironmentSatisfaction)
set.seed(12)
splitPercent = 0.80
trainIndex = sample(1:dim(data.nb)[1],round(splitPercent * dim(data.nb)[1]))
train.nb = data.nb[trainIndex,]
test.nb = data.nb[-trainIndex,]
model.nb = naiveBayes(Attrition~.,data=train.nb, laplace = 1)
predict.nb = predict(model.nb,test.nb)
table(predict.nb,test.nb$Attrition)
confusionMatrix(predict.nb,test.nb$Attrition)
#Load library to run stepwise regression method to choose an optimal simple model
library(MASS)
#Build the model with internel verfication
set.seed(24)
train.control <- trainControl(method = "cv", number = 10)
step.model = train(MonthlyIncome~., data=employeeData,
method="lmStepAIC",
trControl = train.control,
trace=FALSE)
#Model Accuracy
step.model$results
step.model$finalModel
summary(step.model$finalModel)
?sd
?pt
pt(0.95,49)
qt(0.95,49)
?qt
qt(0.975,473.85)
qnorm(0.975,13)
?qnorm
qnorm(0.975,0,1)
?pf
pt(1.06,1,280)
pt(5,1,280)
pf(1.06,1,280)
1- pf(1.06,1,280)
?pt
pt(2.84,14)
1-pt(2.84,14)
1-2*pt(2.84,14)
(1-pt(2.84,14))*2
qnorm(2.84,0,1)
?qnorm
qnorm(2.84)
pnorm(2.84)
1- pnorm(2.84)
2^(1.249-2.594*0.031)
pf(1.06,1,280)
qt(0.95,1000000000)
qnorm(0.95)
qt(0.95,100)
qt(0.95,30)
qt(0.025,30)
qt(0.025,30)
qnorm(0.025,30)
qnorm(0.025)
qt(0.025,100)
qt(0.025,1000000)
qt(0.025,10000000000)
qt(0.45,1000000000000)
qnorm(0.45)
qnorm(0.5)
qt(0.5,1000000000000)
qnorm(0.6)
qt(0.6,1000)
qt(0.6,100)
exp(0.1975)
9.381-8.667
0.714/9
0.079/0.619
pf(0.128,9,14)
1-pf(0.128,9,14)
1.99979-0.001126
0.0998499+0.0102828
1.99979-0.0001084
0.0998499+0.0303267
exp(1.999789181)
Exp(-0.001083732)
exp(-0.001083732)
exp(0.03032665)
231-7
?qt
qt(0.995,224)
0.200127253+0.00013768*2.598
0.200127253-0.00013768*2.598
qt(0.975,224)
1.998664+0.1101327*9.4+0.2001273*2
exp(3.424166)
1.998664+0.1101327*9.4+0.2001273*3
exp(3.634293)
exp(3.627926)
exp(3.640658)
0.010282784-0.030326651
(4+9-8.2)*10^08
sqrt(4.8*10^(-8))
qt(0.975,224)
-0.02004387+1.97*0.000219
-0.02004387-1.97*0.000219
0.714/5
9.381-9.667
9.381-8.667
0.1428/0.619
1-pf(0.231,5,14)
df = c(1,2,3,4,5)
scale(df)
-4.66334+(-.2)*(-26.14277)+(-.2)*18.39440
-1.10386+(-.2)*(-6.82377)+(-.2)*1.27017
-4.66334+(-.2)*(-26.14277)+(0)*18.39440
-1.10386+(-.2)*(-6.82377)+(0)*1.27017
-4.66334+(-.3)*(-26.14277)+(0)*18.39440
-1.10386+(-.3)*(-6.82377)+(0)*1.27017
2.7+2.3*5
2.7+2.3*8
2.7+2.3*25
2.7+2.3*30
((14.2-14)^2+(21.1-22)^2+(60.2-61)^2+(71.7-71)^2)/4
pnorm
?pnorm
pnorm(2.924,0,1)
1-pnorm(2.924,0,1)
2*(1-pnorm(2.924,0,1))
library(MASS)
library(mvtnorm)
library(mvtnorm)
set.seed(1234)
muYes<-c(10,10)
muNo<-c(8,8)
Sigma<-matrix(c(1,.8,.8,1),2,2,byrow=T)
nY<-30
nN<-30
dataYes<-rmvnorm(nY,muYes,Sigma)
dataNo<- rmvnorm(nN,muNo,Sigma)
train<-rbind(dataYes,dataNo)
train<-data.frame(train)
for (i in 3:20){
train<-cbind(train,rnorm(nY+nN))
}
names(train)<-paste("X",1:20,sep="")
train$Response<-rep(c("Yes","No"),each=30)
train$Response<-factor(train$Response)
#Creating a test set
muYes<-c(10,10)
muNo<-c(8,8)
Sigma<-matrix(c(1,.8,.8,1),2,2,byrow=T)
nY<-500
nN<-500
dataYes<-rmvnorm(nY,muYes,Sigma)
dataNo<- rmvnorm(nN,muNo,Sigma)
test<-rbind(dataYes,dataNo)
test<-data.frame(test)
for (i in 3:20){
test<-cbind(test,rnorm(nY+nN))
}
names(test)<-paste("X",1:20,sep="")
test$Response<-rep(c("Yes","No"),each=500)
test$Response<-factor(test$Response)
mylda<-lda(Response~X1+X2+X3+X4+X5+X6+X7+X8+X9+X10,data=train)
pred<-predict(mylda,newdata=test)$class  #Predictions can come in many forms, the class form provides the categorical level of your response.
Truth<-test$Response
x<-table(pred,Truth) # Creating a confusion matrix
x
#Missclassification Error
ME<-(x[2,1]+x[1,2])/1000
ME
#Calculating overall accuracy
1-ME
mylda<-lda(Response~.,data=train)
mylda
mylda<-lda(Response~X1+X2+X3+X4+X5+X6+X7+X8+X9+X10,data=train)
mylda
.5+1.96*sqrt(.5*.5/1000)
.5+1.96*sqrt(.5*.5/1000)
0.516129-0.311475
19*30/42*32
19*30/42/32
32*42/19/30
sqrt(0.516129*(1-0.516129)/62+0.311475*(1-0.311475)/61)
0.08685489*1.96
0.204654-0.1702356
0.204654+0.1702356
log(2.36)
sqrt(1/32+1/30+1/19+1/42)
0.86-1.96*0.3755
0.86+1.96*0.3755
exp(0.124)
exp(1.596)
?pnorm
(32+19)/(62+61)
sqrt(0.4146*(1-0.4146)/61+0.4146*(1-0.4146)/62)
0.204654/0.08884496
pnorm(2.3035,0,1)
1-pnorm(2.3035,0,1)
(1-pnorm(2.3035,0,1))*2
0.86/0.3755
(1-pnorm(2.29,0,1))*2
(76/411)/(105/407)
76*302/105/335
(302/407)/(335/411)
(335/411)/(302/407)
(335/76)/(302/105)
# You need to adjust the work environment to run this code
setwd("/Users/mingyang/Desktop/SMU/File Management & Database/7330-Term-Porject/Data Set")
library(tidyverse)
# text mining library
library(tidytext)
#plotting and pipes
library(ggplot2)
# Read in data under tweets
tweets = read_csv("vaccination_tweets0313.csv")
str(tweets)
dim(tweets)
#Look at data
library(visdat)
vis_dat(tweets)
# Look at missing Data
library(naniar)
colSums(is.na(tweets))
gg_miss_var(tweets)
clean_text = function(string){
#Remove URL
string = gsub("\\s?(f|ht)(tp)(s?)(://)([^\\.]*)[\\.|/](\\S*)","", string)
#Remove \r and \n
string = gsub("\r?\n|\r"," ",string)
return(string)
}
# Performance regex on tweets$text column
tweets$text = clean_text(tweets$text)
# Uniform Date Format
library(lubridate)
tweets$date = ymd_hms(tweets$date)
# Write just tweets comments column out to a csv
write.csv(tweets$text, "tweet_posts0313.csv", row.names=F)
tweet1 = read.csv("tweet_posts.csv",header = FALSE)
dim(tweet1)
tweet1 = read.csv("tweet_posts0313.csv",header = FALSE)
dim(tweet1)
dim(tweets)
tweet1 = read.csv("tweet_posts0313.csv",header = FALSE)
dim(tweet1)
# Decide to fix USA after fixing STATES name first
usa.states = read_csv("usa_states.csv")
usa.states = data.frame(usa.states)
str(usa.states)
usa.states$State = toupper(usa.states$State)
usa.states = usa.states[,-2]
head(usa.states)
tweets$user_location = toupper(tweets$user_location)
head(tweets$user_location)
usstatelength = dim(usa.states)[1]
usstatelength
tweets$user_state = as.character(NA)
#Make state name decending
usa.states = usa.states %>% mutate(statenamelength = nchar(State)) %>%arrange(desc(statenamelength))
head(usa.states)
usa.states
tweetslength = dim(tweets)[1]
tweetslength
str(tweets)
usa.states[1,1]
# Go through tweets for all states
for(i in 1:tweetslength){
for(j in 1:usstatelength){
if(grepl(usa.states[j,1],tweets[i,3])){
print(paste("fixing tweet",i))
tweets[i,17]=usa.states[j,1]
print(paste("fixed",tweets[i,3],tweets[i,17]))
break
}
}
}
usa.states[1,2]
usa.states[2,2]
# WASHINGTON DC fixed to WASHINGTON - should be fixed when doing " StateCode$" structure
# KANSAS CITY, MO  -- went to KANSAS - should be fixed when doing " StateCode$" structure
# Now go through City, Statecode structure
for(i in 1:tweetslength){
for(j in 1:usstatelength){
current.state = paste(" ",usa.states[j,2],"$",sep = "")
if(grepl(current.state,tweets[i,3])){
print(paste("fixing tweet",i))
tweets[i,17]=usa.states[j,1]
print(paste("fixed",tweets[i,3],tweets[i,17]))
break
}
}
}
usa.states[1,1]
# WASHINGTON D.C. need to be treated as special case
for(i in 1:tweetslength){
if((!is.na(tweets[i,17]))&(tweets[i,17]=="WASHINGTON")){
if(grepl("D.C.",tweets[i,3])){
print(paste("fixing tweet",i))
tweets[i,17]=usa.states[1,1]
print(paste("fixed",tweets[i,3],tweets[i,17]))
break
}
}
}
location.and.state = tweets%>%filter(!is.na(user_state))%>%select(user_location,user_state)
view(location.and.state)
location.and.state = tweets%>%filter(is.na(user_state))%>%select(user_location,user_state)
view(location.and.state)
location.and.state = tweets%>%filter(is.na(user_state)&!is.na(user_location))%>%select(user_location,user_state)
view(location.and.state)
location.and.state = tweets%>%filter(is.na(user_state)&!is.na(user_location)&grepl("NYC",user_location))%>%select(user_location,user_state)
view(location.and.state)
usa.states
# Fixing NYC in user_location
for(i in 1:tweetslength){
for(j in 1:usstatelength){
if(!is.na(tweets[i,3])&grepl("NYC",tweets[i,3])&(!is.na(tweets[i,17]))){
print(paste("fixing tweet",i))
tweets[i,17]="NEW YORK"
print(paste("fixed",tweets[i,3],tweets[i,17]))
break
}
}
}
# Fixing NYC in user_location
for(i in 1:tweetslength){
for(j in 1:usstatelength){
if(grepl("NYC",tweets[i,3])&(!is.na(tweets[i,17]))){
print(paste("fixing tweet",i))
tweets[i,17]="NEW YORK"
print(paste("fixed",tweets[i,3],tweets[i,17]))
break
}
}
}
location.and.state = tweets%>%filter(is.na(user_state)&!is.na(user_location)&grepl("NYC",user_location))%>%select(user_location,user_state)
view(location.and.state)
# Fixing NYC in user_location
for(i in 1:tweetslength){
if(grepl("NYC",tweets[i,3])&(!is.na(tweets[i,17]))){
print(paste("fixing tweet",i))
tweets[i,17]="NEW YORK"
print(paste("fixed",tweets[i,3],tweets[i,17]))
break
}
}
location.and.state = tweets%>%filter(is.na(user_state)&!is.na(user_location)&grepl("NYC",user_location))%>%select(user_location,user_state)
view(location.and.state)
# Fixing NYC in user_location
for(i in 1:tweetslength){
if(grepl("NYC",tweets[i,3])&(is.na(tweets[i,17]))){
print(paste("fixing tweet",i))
tweets[i,17]="NEW YORK"
print(paste("fixed",tweets[i,3],tweets[i,17]))
break
}
}
location.and.state = tweets%>%filter(is.na(user_state)&!is.na(user_location)&grepl("NYC",user_location))%>%select(user_location,user_state)
view(location.and.state)
# Fixing NYC in user_location
for(i in 1:tweetslength){
if(grepl("NYC",tweets[i,3])&(is.na(tweets[i,17]))){
print(paste("fixing tweet",i))
tweets[i,17]="NEW YORK"
print(paste("fixed",tweets[i,3],tweets[i,17]))
}
}
location.and.state = tweets%>%filter(is.na(user_state)&!is.na(user_location)&grepl("NYC",user_location))%>%select(user_location,user_state)
view(location.and.state)
#Adding state code
tweets$us_state_code= as.character(NA)
usa.states[2,2]
#Adding USA state code for easier merging data.
for(i in 1:tweetslength){
if(!is.na(tweets[i,17])){
for(j in 1:usstatelength){
if(usa.states[j,1]==tweets[i,17]){
print(paste("fixing tweet",i))
tweets[i,18]=usa.states[j,2]
print(paste("fixed",tweets[i,3],tweets[i,19],tweets[i,20]))
break
}
}
}
}
#Adding state code
tweets$us_state_code= as.character(NA)
str(tweets)
#Adding USA state code for easier merging data.
for(i in 1:tweetslength){
if(!is.na(tweets[i,17])){
for(j in 1:usstatelength){
if(usa.states[j,1]==tweets[i,17]){
print(paste("fixing tweet",i))
tweets[i,18]=usa.states[j,2]
print(paste("fixed",tweets[i,3],tweets[i,17],tweets[i,18]))
break
}
}
}
}
location.and.state = tweets%>%filter(!is.na(user_state))%>%select(user_state,user_state_code)
location.and.state = tweets%>%filter(!is.na(user_state))%>%select(user_state,us_state_code)
view(location.and.state)
# Look at TOP US STATES
tweets %>%
count(user_state,sort = TRUE) %>%
mutate(user_state = reorder(user_state, n)) %>%
na.omit()%>%
top_n(20)%>%
ggplot(aes(x=user_state,y=n))+
geom_col()+
coord_flip()+
labs(x = "Count",y="Location",title="Where Twitter users are from - unique State")
gg_miss_var(tweets)
###########################################################################################
# write out location data with US state fixed
write.csv(tweets,"fixed_location_tweets0313.csv",row.names=FALSE)
dim(tweets)
