---
title: "EDA_by_Nick"
author: "Mingyang Nick YU"
date: "2/24/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### Import Data called tweets_loc_sen0223.csv and start analysis and plotting
#testing commit signiture

```{r}
# You need to adjust the work environment to run this code
setwd("/Users/mingyang/Desktop/SMU/File Management & Database/7330-Term-Porject/Data Set")
library(tidyverse)
#plotting and pipes 
library(ggplot2)
# text mining library
library(tidytext)
#Look at data
library(visdat)
# Look at missing Data
library(naniar)

tweets = read_csv("tweets_loc_sen0223.csv")
tweets = data.frame(tweets)
# Look at missing data
gg_miss_var(tweets)
# Look at structure of tweets
str(tweets)
# Look at TOP US STATES
tweets %>% 
  count(user_state,sort = TRUE) %>%
  mutate(user_state = reorder(user_state, n)) %>%
  na.omit()%>%
  top_n(20)%>%
  ggplot(aes(x=user_state,y=n))+
  geom_col()+
  coord_flip()+
  labs(x = "Count",y="Location",title="Where Twitter users are from - unique State")

# Look at TOP WORLD CITIES
tweets %>% 
  count(user_city,sort = TRUE) %>%
  mutate(user_city = reorder(user_city, n)) %>%
  na.omit()%>%
  top_n(20)%>%
  ggplot(aes(x=user_city,y=n))+
  geom_col()+
  coord_flip()+
  labs(x = "Count",y="Location",title="Where Twitter users are from - unique City")

# Look at TOP Countries
tweets %>% 
  count(user_country,sort = TRUE) %>%
  mutate(user_country = reorder(user_country, n)) %>%
  na.omit()%>%
  top_n(20)%>%
  ggplot(aes(x=user_country,y=n))+
  geom_col()+
  coord_flip()+
  labs(x = "Count",y="Location",title="Where Twitter users are from - unique Country")

# Make Sentiment into factor
tweets$sentiment = as.factor(tweets$sentiment)

# Look at USA sentiment
usa_sentiment = tweets %>% filter(user_country=="UNITED STATES")%>%select(user_country,sentiment)

ggplot(data = usa_sentiment) + 
  geom_bar(mapping = aes(x = sentiment, y = stat(prop), group = 1))+
  ggtitle("United States Pfizer Vaccine Sentiment")

# Look at Sentiment in US by States
tweets %>% filter(!is.na(user_state)) %>% 
  select(user_state,sentiment) %>%
  ggplot(aes(user_state, ..count..))+
  geom_bar(aes(fill=sentiment))+coord_flip()+ggtitle("Sentiment across all US States")

# Look at Sentiment in the world by countries
tweets %>% filter(!is.na(user_country)) %>%
  filter(user_country=="UNITED STATES"|user_country=="UNITED KINGDOM"|
           user_country=="CANADA"|user_country=="INDIA"|
           user_country=="UNITED ARAB EMIRATES"|user_country=="IRELAND"|
           user_country=="FRANCE"|user_country=="GERMANY")%>%
  mutate(user_country = fct_relevel(user_country, 
            "GERMANY", "FRANCE", "IRELAND", 
            "UNITED ARAB EMIRATES", "INDIA", "CANADA", 
            "UNITED KINGDOM", "UNITED STATES")) %>%
  select(user_country,sentiment) %>%
  ggplot(aes(user_country, ..count..))+
  geom_bar(aes(fill=sentiment))+coord_flip()+
  ggtitle("Sentiment across top 8 country by tweets")

# Look at Sentiment in the US by states - top 8 states tweets
tweets %>% filter(!is.na(user_state)) %>%
  filter(user_state=="CALIFORNIA"|user_state=="NEW YORK"|
           user_state=="TEXAS"|user_state=="FLORIDA"|
           user_state=="WASHINGTON"|user_state=="ILLINOIS"|
           user_state=="GEORGIA"|user_state=="MICHIGAN")%>%
  mutate(user_state = fct_relevel(user_state, 
            "MICHIGAN", "GEORGIA", "ILLINOIS", 
            "WASHINGTON", "FLORIDA", "TEXAS", 
            "NEW YORK", "CALIFORNIA")) %>%
  select(user_state,sentiment) %>%
  ggplot(aes(user_state, ..count..))+
  geom_bar(aes(fill=sentiment))+coord_flip()+
  ggtitle("Sentiment across top 8 US states by tweets")

# Look at Sentiment for top 8 cities by tweets
tweets %>% filter(!is.na(user_city)) %>%
  filter(user_city=="LONDON"|user_city=="NEW YORK"|
           user_city=="DUBAI"|user_city=="NEW DELHI"|
           user_city=="TORONTO"|user_city=="MUMBAI"|
           user_city=="GLASGOW"|user_city=="CALIFORNIA")%>%
  mutate(user_city = fct_relevel(user_city, 
            "CALIFORNIA", "GLASGOW", "MUMBAI", 
            "TORONTO", "NEW DELHI", "DUBAI", 
            "NEW YORK", "LONDON")) %>%
  select(user_city,sentiment) %>%
  ggplot(aes(user_city, ..count..))+
  geom_bar(aes(fill=sentiment))+coord_flip()+
  ggtitle("Sentiment across top 8 world cities by tweets")

```

### Attempt World Cloud for hashtags column

```{r}
library(stringr)
library(tm)
library(SnowballC)
library(wordcloud)
library(RColorBrewer)
#text =  "['coronavirus', 'SputnikV', 'AstraZeneca', 'PfizerBioNTech', 'Moderna', 'Covid_19']"
hashtags = str_extract_all(tweets$hashtags,"\'.*\'")
hashtags = unlist(hashtags)
hashtags = hashtags %>% na.omit()
# get rid of single quotes
hashtags = gsub("'", '', hashtags)

# create a corpus
hashtags_corpus = Corpus(VectorSource(hashtags))

# Convert the text to lower case
hashtags_corpus <- tm_map(hashtags_corpus, content_transformer(tolower))

# remove custom stop words
custom.stop.words = c("pfizerbiontech","covidvaccine","vaccine","covid19")
hashtags_corpus = tm_map(hashtags_corpus,removeWords,custom.stop.words)

#inspect(hashtags_corpus)

# create document term matrix applying some transformations
tdm = TermDocumentMatrix(hashtags_corpus,
   control = list(removePunctuation = TRUE))

# define tdm as matrix
m = as.matrix(tdm)

# get word counts in decreasing order
word_freqs = sort(rowSums(m), decreasing=TRUE) 

# create a data frame with words and their frequencies
dm = data.frame(word=names(word_freqs), freq=word_freqs)

# plot wordcloud
set.seed(100)
wordcloud(dm$word, dm$freq, random.order=FALSE, random.color=TRUE, colors=brewer.pal(8, "Dark2"))


```

### Observe Positive tweets hashtag key words

```{r}

pos.tweets = tweets %>% filter(sentiment=="POSITIVE")
hashtags = str_extract_all(pos.tweets$hashtags,"\'.*\'")
hashtags = unlist(hashtags)
hashtags = hashtags %>% na.omit()
# get rid of single quotes
hashtags = gsub("'", '', hashtags)

# create a corpus
hashtags_corpus = Corpus(VectorSource(hashtags))

# Convert the text to lower case
hashtags_corpus <- tm_map(hashtags_corpus, content_transformer(tolower))
# remove custom stop words
custom.stop.words = c("pfizerbiontech","covidvaccine","vaccine","covid19")
hashtags_corpus = tm_map(hashtags_corpus,removeWords,custom.stop.words)

#inspect(hashtags_corpus)

# create document term matrix applying some transformations
tdm = TermDocumentMatrix(hashtags_corpus,
   control = list(removePunctuation = TRUE))

# define tdm as matrix
m = as.matrix(tdm)

# get word counts in decreasing order
word_freqs = sort(rowSums(m), decreasing=TRUE) 

# create a data frame with words and their frequencies
dm = data.frame(word=names(word_freqs), freq=word_freqs)

# plot wordcloud
set.seed(105)
wordcloud(dm$word, dm$freq, random.order=FALSE, random.color=TRUE, colors=brewer.pal(8, "Dark2"))

```

### Observe NEGATIVE tweets hashtag key words

```{r}

neg.tweets = tweets %>% filter(sentiment=="NEGATIVE")
hashtags = str_extract_all(neg.tweets$hashtags,"\'.*\'")
hashtags = unlist(hashtags)
hashtags = hashtags %>% na.omit()
# get rid of single quotes
hashtags = gsub("'", '', hashtags)

# create a corpus
hashtags_corpus = Corpus(VectorSource(hashtags))
# Convert the text to lower case
hashtags_corpus <- tm_map(hashtags_corpus, content_transformer(tolower))
# remove custom stop words
custom.stop.words = c("pfizerbiontech","covidvaccine","vaccine","covid19")
hashtags_corpus = tm_map(hashtags_corpus,removeWords,custom.stop.words)

# create document term matrix applying some transformations
tdm = TermDocumentMatrix(hashtags_corpus,
   control = list(removePunctuation = TRUE))

# define tdm as matrix
m = as.matrix(tdm)

# get word counts in decreasing order
word_freqs = sort(rowSums(m), decreasing=TRUE) 

# create a data frame with words and their frequencies
dm = data.frame(word=names(word_freqs), freq=word_freqs)

# plot wordcloud
set.seed(103)
wordcloud(dm$word, dm$freq, random.order=FALSE, random.color=TRUE, colors=brewer.pal(8, "Dark2"))

```

### Observe NEUTRAL tweets hashtag key words

```{r}

neutral.tweets = tweets %>% filter(sentiment=="NEUTRAL")
hashtags = str_extract_all(neutral.tweets$hashtags,"\'.*\'")
hashtags = unlist(hashtags)
hashtags = hashtags %>% na.omit()
# get rid of single quotes
hashtags = gsub("'", '', hashtags)

# create a corpus
hashtags_corpus = Corpus(VectorSource(hashtags))

# Convert the text to lower case
hashtags_corpus <- tm_map(hashtags_corpus, content_transformer(tolower))
# remove custom stop words
custom.stop.words = c("pfizerbiontech","covidvaccine","vaccine","covid19")
hashtags_corpus = tm_map(hashtags_corpus,removeWords,custom.stop.words)

# create document term matrix applying some transformations
tdm = TermDocumentMatrix(hashtags_corpus,
   control = list(removePunctuation = TRUE))

# define tdm as matrix
m = as.matrix(tdm)

# get word counts in decreasing order
word_freqs = sort(rowSums(m), decreasing=TRUE) 

# create a data frame with words and their frequencies
dm = data.frame(word=names(word_freqs), freq=word_freqs)

# plot wordcloud
set.seed(103)
wordcloud(dm$word, dm$freq, random.order=FALSE, random.color=TRUE, colors=brewer.pal(8, "Dark2"))

```

### Observe MIXED tweets hashtag key words

```{r}

mixed.tweets = tweets %>% filter(sentiment=="MIXED")
hashtags = str_extract_all(mixed.tweets$hashtags,"\'.*\'")
hashtags = unlist(hashtags)
hashtags = hashtags %>% na.omit()
# get rid of single quotes
hashtags = gsub("'", '', hashtags)

# create a corpus
hashtags_corpus = Corpus(VectorSource(hashtags))
# Convert the text to lower case
hashtags_corpus <- tm_map(hashtags_corpus, content_transformer(tolower))
# remove custom stop words
custom.stop.words = c("pfizerbiontech","covidvaccine","vaccine","covid19")
hashtags_corpus = tm_map(hashtags_corpus,removeWords,custom.stop.words)

# create document term matrix applying some transformations
tdm = TermDocumentMatrix(hashtags_corpus,
   control = list(removePunctuation = TRUE))

# define tdm as matrix
m = as.matrix(tdm)

# get word counts in decreasing order
word_freqs = sort(rowSums(m), decreasing=TRUE) 

# create a data frame with words and their frequencies
dm = data.frame(word=names(word_freqs), freq=word_freqs)

# plot wordcloud
set.seed(103)
wordcloud(dm$word, dm$freq, random.order=FALSE, random.color=TRUE, colors=brewer.pal(8, "Dark2"))

```

# from Nicole

```{r}
# use tweet_posts.csv that Nick cleaned for the sentiment analysis
library("tm")
library("SnowballC")
library("wordcloud")
library("RColorBrewer")
tweetCol = read_csv("tweet_posts.csv")
docs = Corpus(VectorSource(tweetCol))
inspect(docs)
toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
docs <- tm_map(docs, toSpace, "/")
docs <- tm_map(docs, toSpace, "@")
docs <- tm_map(docs, toSpace, "\\|")
# Remove english common stopwords
docs <- tm_map(docs, removeWords, stopwords("english"))
# Remove punctuations
docs <- tm_map(docs, removePunctuation)
# Eliminate extra white spaces
docs <- tm_map(docs, stripWhitespace)
# Remove your own stop word
# specify your stopwords as a character vector
docs <- tm_map(docs, removeWords, c("...")) 

dtm <- TermDocumentMatrix(docs)
m <- as.matrix(dtm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
head(d, 10)
set.seed(1234)
wordcloud(words = d$word, freq = d$freq, min.freq = 1,
          max.words=200, random.order=FALSE, rot.per=0.35, 
          colors=brewer.pal(8, "Dark2"))
# Note: the window needs to be big enough to print all the words.
# It prints fully when I use a regular .R file and stretch out the plot area

```











