---
title: "Pfizer Vaccine Tweets"
author: "Nicole Norelli, Mingyang Nick YU & Justin Ehly"
date: "2/3/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Pfizer Vaccine Tweets: A Sentiment Analysis and Implications for Public Health
#### Problem Statement:
Worldwide distribution of the Pfizer vaccine in response to the COVID-19 pandemic has been inconsistent, in part due to insufficient data on public perceptions of the vaccine. Deep analysis of public sentiment could assist the medical community develop a more effective distribution strategy, which would save lives.
This project will analyze real-world Twitter data related to the Pfizer vaccine to identify public perceptions of the vaccine as well as any trends that might be useful to researchers and government officials. We will use these trends to provide recommendations that could inform world-wide government actors to facilitate distribution, education, and public awareness to bolster confidence in the vaccine. The dataset is provided by Kaggle, and the resource link is listed below.

```{r}
# You need to adjust the work environment to run this code
setwd("/Users/mingyang/Desktop/SMU/File Management & Database/7330-Term-Porject/Data Set")
# Resources started using:
# https://www.earthdatascience.org/courses/earth-analytics/get-data-using-apis/use-twitter-api-r/
library(tidyverse)
#plotting and pipes 
library(ggplot2)
# text mining library
library(tidytext)
# Read in data under tweets
tweets = read_csv("vaccination_tweets.csv")
str(tweets)
summary(tweets)
#Look at data
library(visdat)
vis_dat(tweets)
# Look at missing Data
library(naniar)
colSums(is.na(tweets))
gg_miss_var(tweets)

library(GGally)
tweets %>% ggcorr(label=TRUE)
# Look at locations
summary(tweets$user_location)
# how many locations are represented
length(unique(tweets$user_location))
## [1] 1356
# Sort by count
tweets %>% 
  count(user_location,sort = TRUE) %>%
  mutate(user_location = reorder(user_location, n)) %>%
  na.omit()%>%
  top_n(20)%>%
  ggplot(aes(x=user_location,y=n))+
  geom_col()+
  coord_flip()+
  labs(x = "Count",y="Location",title="Where Twitter users are from - unique locations")
# Further location cleaning is needed...
# Need cleaning for user_location column
# 

world.cities = read_csv("worldcities.csv")
str(world.cities)
world.cities %>% filter(city != city_ascii)

#Adding user_city and user_country column for tweets data
tweets['user_city'] <- as.character(NA)
tweets['user_country'] <- as.character(NA)
#transfer user location to all upper case
tweets$user_location = toupper(tweets$user_location)
#transfer world.cities city, city_ascii,country to upper
world.cities$city = toupper(world.cities$city)
world.cities$city_ascii = toupper(world.cities$city_ascii)
world.cities$country = toupper(world.cities$country)

citylength = dim(world.cities)[1] #26569
tweetslength = dim(tweets)[1] #4139
# The following for loop is for filling in user_city and user_country columns when 
# There is a match of city name in user_location
#install.packages("svMisc")
tweets = data.frame(tweets)
world.cities = data.frame(world.cities)
# Reorganize world.cities with city names in decending order
world.cities = world.cities %>% mutate(citynamelength = nchar(city_ascii)) %>%arrange(desc(citynamelength))
head(world.cities)
for(i in 1:tweetslength){
  for(j in 1:citylength) {
    if(grepl(world.cities[j,1],tweets[i,3])){
      print(paste("Processing Tweet ",i))
      tweets[i,17] = world.cities[j,2]
      tweets[i,18] = world.cities[j,5]
      print(paste("fixed",tweets[i,17],", ",tweets[i,18]))
      break
    } 
    else if(grepl(world.cities[j,2],tweets[i,3])){
      print(paste("Processing Tweet",i))
      tweets[i,17] = world.cities[j,2]
      tweets[i,18] = world.cities[j,5]
      print(paste("fixed",tweets[i,17],",",tweets[i,18]))
      break
    }
  }
}
# Explore how well is the city matching did
# smaller city names seems to problematic
tweets$user_country = ifelse(tweets$user_city=="SANDY","UNITED KINGDOM",tweets$user_country)

tweets %>% filter(nchar(user_city)<=4&nchar(user_location)<35)%>%select(id,user_location,user_city,user_country)

# We can see some country are wrongly labeled. Using country dataset to correct these errors above
#write to csv just so not needing to redo this for loop above
write.csv(tweets,"fixed_city_name_tweets.csv",row.names = FALSE)
#leftover = tweets%>%
#  filter(is.na(user_city)&(!is.na(user_location)))%>%
#                             select(id,user_location,user_city,user_country)
#Write out not fixed column and see what's going on...
#write.csv(leftover,"after_city_name_fix.csv",row.names=FALSE)
##########################################################################################
#re-read in tweets from cleaned data
tweets = read_csv("fixed_city_name_tweets.csv")
tweets = data.frame(tweets)
summary(tweets)
# Look again at user_city
tweets %>% 
  count(user_city,sort = TRUE) %>%
  mutate(user_city = reorder(user_city, n)) %>%
  na.omit()%>%
  top_n(20)%>%
  ggplot(aes(x=user_city,y=n))+
  geom_col()+
  coord_flip()+
  labs(x = "Count",y="Location",title="Where Twitter users are from - unique cities")

# read in country list
countrylist = read.csv("countries.csv")
# ENGLAND not in the country name list - Fix first
for(i in 1:tweetslength){
  if(tweets[i,17]=="GLAND"&grepl("ENGLAND",tweets[i,3])){
    print(paste("fixing tweet",i))
    tweets[i,17]=as.character(NA)
    tweets[i,18]="UNITED KINGDOM"
    print(paste("fixed",tweets[i,3],tweets[i,18]))
  }
}

countrylength = dim(countrylist)[1]
# Now go through full country names first
for(i in 1:tweetslength){
  for(j in 1:countrylength){
    if(grepl(countrylist[j,2],tweets[i,3])&is.na(tweets[i,18])){
      print(paste("fixing tweet",i))
      tweets[i,18]=countrylist[j,2]
      print(paste("fixed",tweets[i,3],tweets[i,18]))
      break
    }
    else if(grepl(countrylist[j,2],tweets[i,3])&(countrylist[j,2]!=tweets[i,18])){
      print(paste("fixing tweet",i))
      tweets[i,17]=as.character(NA)
      tweets[i,18]=countrylist[j,2]
      print(paste("fixed",tweets[i,3],tweets[i,18]))
      break
    }
    
  }
}
# Now observe again for user_city
tweets %>% 
  count(user_city,sort = TRUE) %>%
  mutate(user_city = reorder(user_city, n)) %>%
  na.omit()%>%
  top_n(20)%>%
  ggplot(aes(x=user_city,y=n))+
  geom_col()+
  coord_flip()+
  labs(x = "Count",y="Location",title="Where Twitter users are from - unique cities")
# Observe for user_country
tweets %>% 
  count(user_country,sort = TRUE) %>%
  mutate(user_country = reorder(user_country, n)) %>%
  na.omit()%>%
  top_n(20)%>%
  ggplot(aes(x=user_country,y=n))+
  geom_col()+
  coord_flip()+
  labs(x = "Count",y="Location",title="Where Twitter users are from - unique cities")
# Looks like IG, ARTH, YE, VAIL, OB, AS needs to be fixed
tweets %>% filter(user_city=="IG")%>% select(id,user_location,user_city,user_country)
#fixing Michigan, USA
for(i in 1:tweetslength){
  if(tweets[i,17]=="IG"&grepl("MICHIGAN, USA",tweets[i,3])){
    print(paste("fixing tweet",i))
    tweets[i,17]=as.character(NA)
    tweets[i,18]="UNITED STATES"
    print(paste("fixed",tweets[i,3],tweets[i,18]))
  }
}
#fixing Michigan
for(i in 1:tweetslength){
  if(tweets[i,17]=="IG"&grepl("MICHIGAN",tweets[i,3])&(!is.na(tweets[i,17]))){
    print(paste("fixing tweet",i))
    tweets[i,17]=as.character(NA)
    tweets[i,18]="UNITED STATES"
    print(paste("fixed",tweets[i,3],tweets[i,18]))
  }
}
# Observe again
tweets %>% filter(user_city=="IG")%>% select(id,user_location,user_city,user_country)
# Rest of IG don't have location
for(i in 1:tweetslength){
  if(tweets[i,17]=="IG"&(!is.na(tweets[i,17]))){
    print(paste("fixing tweet",i))
    tweets[i,17]=as.character(NA)
    tweets[i,18]=as.character(NA)
    print(paste("fixed",tweets[i,3]))
  }
}
# Observe again
tweets %>% filter(user_city=="IG")%>% select(id,user_location,user_city,user_country)
# Now look at ARTH
tweets %>% filter(user_city=="ARTH")%>% select(id,user_location,user_city,user_country)
for(i in 1:tweetslength){
  if(tweets[i,17]=="ARTH"&(!is.na(tweets[i,17]))){
    if(grepl("WI",tweets[i,3])){
      print(paste("fixing tweet",i))
      tweets[i,17]=as.character(NA)
      tweets[i,18]="UNITED STATES"
      print(paste("fixed",tweets[i,3],tweets[i,18]))
    }
    else{
      print(paste("fixing tweet",i))
      tweets[i,17]=as.character(NA)
      tweets[i,18]=as.character(NA)
      print(paste("fixed",tweets[i,3]))
    }
    
  }
}
# Now fixing YE
tweets %>% filter(user_city=="YE")%>% select(id,user_location,user_city,user_country)
for(i in 1:tweetslength){
  if(tweets[i,17]=="YE"&(!is.na(tweets[i,17]))){
    if(grepl("SC",tweets[i,3])){
      print(paste("fixing tweet",i))
      tweets[i,17]=as.character(NA)
      tweets[i,18]="UNITED STATES"
      print(paste("fixed",tweets[i,3],tweets[i,18]))
    }
    else{
      print(paste("fixing tweet",i))
      tweets[i,17]=as.character(NA)
      tweets[i,18]="TURKEY"
      print(paste("fixed",tweets[i,3],tweets[i,18]))
    }
  }
}
# Now fixing VAIL
tweets %>% filter(user_city=="VAIL")%>% select(id,user_location,user_city,user_country)
for(i in 1:tweetslength){
  if(tweets[i,17]=="VAIL"&(!is.na(tweets[i,17]))){
    print(paste("fixing tweet",i))
    tweets[i,17]=as.character(NA)
    tweets[i,18]=as.character(NA)
    print(paste("fixed",tweets[i,3]))
  }
}
# Now fixing OB
tweets %>% filter(user_city=="OB")%>% select(id,user_location,user_city,user_country)
for(i in 1:tweetslength){
  if(tweets[i,17]=="OB"&(!is.na(tweets[i,17]))){
    if(grepl("USA",tweets[i,3])){
      print(paste("fixing tweet",i))
      tweets[i,17]=as.character(NA)
      tweets[i,18]="UNITED STATES"
      print(paste("fixed",tweets[i,3],tweets[i,18]))
    }
    else{
      print(paste("fixing tweet",i))
      tweets[i,17]=as.character(NA)
      tweets[i,18]=as.character(NA)
      print(paste("fixed",tweets[i,3]))
    }
  }
}
# Now fixing AS
tweets %>% filter(user_city=="AS")%>% select(id,user_location,user_city,user_country)
######## AS can't be fixed yet... next step is to fix states before the city name can be fixed further###
# Save tweets to csv...
write.csv(tweets,"fixed_city_name_tweets2.csv",row.names = FALSE)
##########################################################################################################
# Load in fixed_city_name_tweets2.csv to continue...
tweets = read_csv("fixed_city_name_tweets2.csv")
tweetslength = dim(tweets)[1] #4139
# Look at user_city less than 4 char
tweets %>% filter(nchar(user_city)<=4&nchar(user_location)<35 & grepl("AMERICA",user_location))%>%select(id,user_location,user_city,user_country)
# Fixing user_city = ICA
for(i in 1:tweetslength){
  if(tweets[i,17]=="ICA"&(!is.na(tweets[i,17]))){
    if(grepl("NORTH AMERICA",tweets[i,3])){
      print(paste("fixing tweet",i))
      tweets[i,17]=as.character(NA)
      tweets[i,18]=as.character(NA)
      print(paste("fixed",tweets[i,3]))
    }
    else if(grepl("AMERICA",tweets[i,3])){
      print(paste("fixing tweet",i))
      tweets[i,17]=as.character(NA)
      tweets[i,18]="UNITED STATES"
      print(paste("fixed",tweets[i,3],tweets[i,18]))
    }
  }
}
# Fixing LA BELLE PROVINCE
for(i in 1:tweetslength){
  if(tweets[i,17]=="BELL"&(!is.na(tweets[i,17]))){
    if(grepl("LA BELLE PROVINCE",tweets[i,3])){
      print(paste("fixing tweet",i))
      tweets[i,17]=as.character(NA)
      tweets[i,18]="CANADA"
      print(paste("fixed",tweets[i,3]))
    }
  }
}

# Decide to fix USA after fixing STATES name first
usa.states = read_csv("usa_states.csv")
usa.states = data.frame(usa.states)
usa.states$State = toupper(usa.states$State)
usa.states = usa.states[,-2]
head(usa.states)
usstatelength = dim(usa.states)[1]
# Finished cleaning usa states
tweets$user_state = as.character(NA)

# Go through tweets for all states
for(i in 1:tweetslength){
  for(j in 1:usstatelength){
    if(grepl(usa.states[j,1],tweets[i,3])&is.na(tweets[i,18])){
      print(paste("fixing tweet",i))
      tweets[i,18]="UNITED STATES"
      tweets[i,19]=usa.states[j,1]
      print(paste("fixed",tweets[i,3],tweets[i,19],tweets[i,18]))
      break
    }
    else if(grepl(usa.states[j,1],tweets[i,3])&tweets[i,18]!="UNITED STATES"){
      print(paste("fixing tweet",i))
      tweets[i,17]=as.character(NA)
      tweets[i,18]="UNITED STATES"
      tweets[i,19]=usa.states[j,1]
      print(paste("fixed",tweets[i,3],tweets[i,19],tweets[i,18]))
      break
    }
  }
}

# write another loop to fix city name
for(i in 1:tweetslength){
  for(j in 1:usstatelength){
    if(grepl(usa.states[j,1],tweets[i,3])&(!is.na(tweets[i,18]))&tweets[i,18]=="UNITED STATES"){
      if((!is.na(tweets[i,17]))&usa.states[j,1]!=tweets[i,17]){
        print(paste("fixing tweet",i))
        tweets[i,17]=as.character(NA)
        tweets[i,18]="UNITED STATES"
        tweets[i,19]=usa.states[j,1]
        print(paste("fixed",tweets[i,3],tweets[i,19],tweets[i,18]))
        break
      }
      else if((!is.na(tweets[i,17]))&usa.states[j,1]==tweets[i,17]){
        print(paste("fixing tweet",i))
        tweets[i,19]=usa.states[j,1]
        print(paste("fixed",tweets[i,3],tweets[i,19]))
        break
      }
    }
  }
}

# Fixing WEST VIRGINIA, MICHIGAN
tweets %>% filter(grepl("WEST VIRGINIA",user_location))%>%select(id,user_location,user_city,user_country,user_state)
for(i in 1:tweetslength){
  if((!is.na(tweets[i,3]))&grepl("WEST VIRGINIA",tweets[i,3])){
    if(grepl("CHARLESTON",tweets[i,3])){
      print(paste("fixing tweet",i))
      tweets[i,17]="CHARLESTON"
      tweets[i,19]="WEST VIRGINIA"
      print(paste("fixed",tweets[i,3],tweets[i,17],tweets[i,19]))
    }
    else{
      print(paste("fixing tweet",i))
      tweets[i,17]=as.character(NA)
      tweets[i,19]="WEST VIRGINIA"
      print(paste("fixed",tweets[i,3],tweets[i,19]))
    }
  }
}
# Look at MICHIGAN
tweets %>% filter(grepl("MICHIGAN",user_location))%>%select(id,user_location,user_city,user_country,user_state)
# Fixing MICHIGAN
for(i in 1:tweetslength){
  if((!is.na(tweets[i,3]))&grepl("MICHIGAN",tweets[i,3])){
    if(grepl("TROY",tweets[i,3])){
      print(paste("fixing tweet",i))
      tweets[i,17]="TROY"
      tweets[i,19]="MICHIGAN"
      print(paste("fixed",tweets[i,3],tweets[i,17],tweets[i,19]))
    }
    else if(grepl("CANTON",tweets[i,3])){
      print(paste("fixing tweet",i))
      tweets[i,17]="CANTON"
      tweets[i,19]="MICHIGAN"
      print(paste("fixed",tweets[i,3],tweets[i,17],tweets[i,19]))
    }
    else {
      print(paste("fixing tweet",i))
      tweets[i,19]="MICHIGAN"
      print(paste("fixed",tweets[i,3],tweets[i,19]))
    }
  }
}

# Fixing remaining USA
for(i in 1:tweetslength){
  if((!is.na(tweets[i,3]))&(!is.na(tweets[i,18]))&grepl("USA",tweets[i,3])&tweets[i,18]!="UNITED STATES"){
    if((!is.na(tweets[i,17]))&tweets[i,17]=="WE"){
      print(paste("fixing tweet",i))
      tweets[i,17]=as.character(NA)
      tweets[i,18]="UNITED STATES"
      tweets[i,19]=as.character(NA)
      print(paste("fixed",tweets[i,3],tweets[i,18]))
    }
    else if((!is.na(tweets[i,17]))&tweets[i,17]=="METRO"){
      print(paste("fixing tweet",i))
      tweets[i,17]="WASHINGTON"
      tweets[i,18]="UNITED STATES"
      tweets[i,19]=as.character(NA)
      print(paste("fixed",tweets[i,3],tweets[i,17],tweets[i,18]))
    }
  }
}
# Fixing CRANBURY, NJ
for(i in 1:tweetslength){
  if((!is.na(tweets[i,3]))&grepl("CRANBURY, NJ",tweets[i,3])){
      print(paste("fixing tweet",i))
      tweets[i,17]="CRANBURY"
      tweets[i,18]="UNITED STATES"
      tweets[i,19]="NEW JERSEY"
      print(paste("fixed",tweets[i,3],tweets[i,17],tweets[i,19],tweets[i,18]))
      break
  }
}

# Fixing NYC where user_city is NA
for(i in 1:tweetslength){
  if((!is.na(tweets[i,3]))&grepl("NYC",tweets[i,3])&is.na(tweets[i,17])){
      print(paste("fixing tweet",i))
      tweets[i,17]="NEW YORK"
      tweets[i,18]="UNITED STATES"
      tweets[i,19]="NEW YORK"
      print(paste("fixed",tweets[i,3],tweets[i,17],tweets[i,19],tweets[i,18]))
  }
}



###########################################################################################
#another write out
write.csv(tweets,"fixed_location_tweets3.csv",row.names=FALSE)
###########################################################################################


```

### Import Data called fixed_location_tweets3.csv and start analysis
```{r}
# You need to adjust the work environment to run this code
setwd("/Users/mingyang/Desktop/SMU/File Management & Database/7330-Term-Porject/Data Set")
library(tidyverse)
#plotting and pipes 
library(ggplot2)
# text mining library
library(tidytext)
#Look at data
library(visdat)
vis_dat(tweets)
# Look at missing Data
library(naniar)

tweets = read_csv("fixed_location_tweets3.csv")
tweets = data.frame(tweets)
# Look at missing data
gg_miss_var(tweets)
# Look at TOP US STATES
tweets %>% 
  count(user_state,sort = TRUE) %>%
  mutate(user_state = reorder(user_state, n)) %>%
  na.omit()%>%
  top_n(20)%>%
  ggplot(aes(x=user_state,y=n))+
  geom_col()+
  coord_flip()+
  labs(x = "Count",y="Location",title="Where Twitter users are from - unique State")

# Look at TOP WORLD CITIES
tweets %>% 
  count(user_city,sort = TRUE) %>%
  mutate(user_city = reorder(user_city, n)) %>%
  na.omit()%>%
  top_n(20)%>%
  ggplot(aes(x=user_city,y=n))+
  geom_col()+
  coord_flip()+
  labs(x = "Count",y="Location",title="Where Twitter users are from - unique City")

# Look at TOP Countries
tweets %>% 
  count(user_country,sort = TRUE) %>%
  mutate(user_country = reorder(user_country, n)) %>%
  na.omit()%>%
  top_n(20)%>%
  ggplot(aes(x=user_country,y=n))+
  geom_col()+
  coord_flip()+
  labs(x = "Count",y="Location",title="Where Twitter users are from - unique Country")

# Look at user_city less than 4 char - potential problem
#temp1 = tweets %>% filter(nchar(user_city)<=4&nchar(user_location)<35)%>%select(id,user_location,user_city,user_country,user_state)
#write.csv(temp1,"observe USA values.csv",row.names=FALSE)

########################################################################################
# Do some regex operation to get rid of urls and "\n" or "\r" new lines under text field
# Text processing - create function to remove url and new line in text
########################################################################################
clean_text = function(string){
  #Remove URL
  string = gsub("\\s?(f|ht)(tp)(s?)(://)([^\\.]*)[\\.|/](\\S*)","", string)
  #Remove \r and \n
  string = gsub("\r?\n|\r"," ",string)
  return(string)
}

# Performance regex on tweets$text column
tweets$text = clean_text(tweets$text)

# Write just tweets comments column out to a csv
write.csv(tweets$text, "tweet_posts.csv", row.names=F)
tweet1 = read.csv("tweet_posts.csv",header = FALSE)
dim(tweet1)
```

### Read in JSON data

```{r}
# Load the package required to read JSON files.
library("rjson")
library(jsonlite)
# Give the input file name to the function.
result <- stream_in(file("sentiment.json"))

# Print the result.
head(result)
result$Sentiment
# Convert JSON file to a data frame.
json_data_frame <- as.data.frame(result)
print(json_data_frame)
dim(json_data_frame)

```













